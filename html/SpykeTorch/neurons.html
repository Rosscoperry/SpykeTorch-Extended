<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>SpykeTorch.neurons API documentation</title>
<meta name="description" content="This modules contains implementations of Spiking Neuron Models: â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>SpykeTorch.neurons</code></h1>
</header>
<section id="section-intro">
<p>This modules contains implementations of Spiking Neuron Models:</p>
<ul>
<li>LIF</li>
<li>EIF</li>
<li>QIF</li>
<li>AdEx</li>
<li>Izhikevich</li>
<li>Heterogeneous Neurons.</li>
</ul>
<p>The Neuron class is the base for all of the other classes and holds few common and useful methods.
Each neuron object holds its own state and computes updates one time-step at a time. This is done by using the <code>__call__</code>
method, which in turn makes the objects <em>callable</em>.
Within this method, a layer of neuron get updated depending on the neuron type and the received input (Post-Synaptic Potentials (PSPs)).
The output of a call to a neuron layer depends on a set of flags, but ultimately includes at least the propagated spikes, which are
the winners selected through a Winner(s)-Take-All (WTA) mechanism.</p>
<p>Within a neuron layer a lateral inhibition system puts neurons in refractory periods. Inhibition can be <em>feature-wise</em> or
<em>location-wise</em>. Feature-wise inhibition will inhibit all the neurons that share the same kernel as the winning one(s).
Location-wise inhibition will inhibit all the neurons that correspond to the same input-location as the winning one(s).</p>
<p>For most of the neurons (all except the <em>Simple</em> ones), the input is expected to be scaled by <span><span class="MathJax_Preview">\frac{1}{t_s}</span><script type="math/tex">\frac{1}{t_s}</script></span>
(see <a href="https://neuronaldynamics.epfl.ch/online/Ch1.S3.html">https://neuronaldynamics.epfl.ch/online/Ch1.S3.html</a>). Therefore, their output spikes are also scaled by this factor.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This modules contains implementations of Spiking Neuron Models:

- LIF
- EIF
- QIF
- AdEx
- Izhikevich
- Heterogeneous Neurons.

The Neuron class is the base for all of the other classes and holds few common and useful methods.
Each neuron object holds its own state and computes updates one time-step at a time. This is done by using the `__call__`
method, which in turn makes the objects _callable_.
Within this method, a layer of neuron get updated depending on the neuron type and the received input (Post-Synaptic Potentials (PSPs)).
The output of a call to a neuron layer depends on a set of flags, but ultimately includes at least the propagated spikes, which are
the winners selected through a Winner(s)-Take-All (WTA) mechanism.

Within a neuron layer a lateral inhibition system puts neurons in refractory periods. Inhibition can be _feature-wise_ or
_location-wise_. Feature-wise inhibition will inhibit all the neurons that share the same kernel as the winning one(s).
Location-wise inhibition will inhibit all the neurons that correspond to the same input-location as the winning one(s).

For most of the neurons (all except the _Simple_ ones), the input is expected to be scaled by \(\\frac{1}{t_s}\)
(see https://neuronaldynamics.epfl.ch/online/Ch1.S3.html). Therefore, their output spikes are also scaled by this factor.

&#34;&#34;&#34;

import torch
import numpy as np
from . import functional as sf
import matplotlib.pyplot as plt
import time
import sys
DEVICE = torch.device(&#39;cuda&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;)

countC = 0
countY = 0

__pdoc__ = {&#34;LIF.__call__&#34;: True,
            &#34;LIF_Simple.__call__&#34;: True,
            &#34;LIF_ode.__call__&#34;: True,
            &#34;EIF.__call__&#34;: True,
            &#34;QIF.__call__&#34;: True,
            &#34;AdEx.__call__&#34;: True,
            &#34;Izhikevich.__call__&#34;: True,}


class Neuron(object):

    def __init__(self, ts=0.01, resting_potential=0.0, v_reset=None, threshold=None, refractory_timesteps=0,
                 inhibition_mode=&#34;feature&#34;):
        self._threshold = threshold

        self.resting_potential = resting_potential
        self.previous_state = None
        self._per_neuron_thresh = None
        self.refractory_timestes = refractory_timesteps
        self.ts = ts
        self.refractoriness = refractory_timesteps*self.ts
        self.refractory_periods = None
        self.v_reset = v_reset if v_reset is not None else resting_potential
        self.inhibition_mode = inhibition_mode

    @property
    def threshold(self):
        return self._threshold

    @threshold.setter
    def threshold(self, threshold):
        self._threshold = threshold

    @property
    def per_neuron_thresh(self):
        return self._per_neuron_thresh

    @per_neuron_thresh.setter
    def per_neuron_thresh(self, value):
        self._per_neuron_thresh = value

    def __str__(self):
        return &#34;Spiking_Neuron_Base_Class&#34;

    def __call__(self, *args, **kwargs):
        raise NotImplementedError()

    def reset(self):
        raise NotImplementedError()

    def get_params(self):
        params = {}
        for k, v in self.__dict__.items():
            if not isinstance(v, torch.Tensor):
                params[k] = v
        return params

    def get_thresholded_potentials(self, current_state):
        &#34;&#34;&#34;
        General method to get thresholded membrane potentials, i.e. a Tensor where values are != 0.0 only if they were above
        the corresponding neuron&#39;s threshold.
        Args:
            current_state (Tensor): membrane potentials of the neurons

        Returns:
            Tensor: thresholded membrane potentials.
        &#34;&#34;&#34;
        thresholded = current_state.clone().detach()

        # inhibit where refractoriness is not consumed
        thresholded[self.refractory_periods &gt; 0] = self.resting_potential

        if self.per_neuron_thresh is None:
            self.per_neuron_thresh = torch.ones(current_state.shape, device=DEVICE)*self.threshold
        if self._threshold is None:
            thresholded[:-1] = 0
        else:
            thresholded[thresholded &lt; self.per_neuron_thresh] = 0.0

        return thresholded

    def _f_ode(self, x, I=0):
        raise NotImplementedError()

    def plot_ode(self, figure: plt.Figure = None, ax: plt.Axes = None, current=None):
        &#34;&#34;&#34;
        Plots the neuron&#39;s ODE. If current is given, ODE with and without current are plotted.
        Multiple plots can be stack onto each other by passing the proper figure and axes as an argument.
        Args:
            figure (pyplot.Figure): Figure to use for plots. If None, a new one is created.
            ax (pyplot.Axes): Axes to use for the plot. If None, a new one is created.
            current: If provided, draws the current ON/OFF plots.

        Returns:
            Tuple: The figure and axes of the plot.
        &#34;&#34;&#34;
        f = False
        if figure is None:
            figure = plt.figure()
            f = True
        if ax is None:
            ax = figure.add_subplot()  # type: plt.Axes

        neuron_name = self.__str__().split(&#34;_&#34;)[0]
        x_points = np.linspace(self.resting_potential, self.threshold + self.threshold / 2, 300)
        y0 = np.array([self._f_ode(x, 0) for x in x_points])
        ax.plot(x_points, y0)
        if current:
            y1 = np.array([self._f_ode(x, current) for x in x_points])
            ax.plot(x_points, y1)
        xmax = ax.get_xlim()[1]
        ax.set_xlim(min(x_points), max(max(x_points), xmax))
        ymin, ymax = ax.get_ylim()
        if f:
            ax.text(self.threshold,  ymin+2, &#34;Vth&#34;,  horizontalalignment=&#39;left&#39;)
            ax.vlines(self.threshold, -5e4, 5e4, &#34;r&#34;, linestyles=&#34;dashed&#34;)
        ax.set_ylim(bottom=min(min(y0), ymin)-5, top=self.threshold)
        ax.hlines(0, min(x_points) - abs(min(x_points) / 4), max(x_points) + abs(max(x_points) / 4), &#34;black&#34;,
                  linewidth=.5)
        leg = ax.get_legend()
        labels = [label._text for label in leg.texts] if leg is not None else []
        l = neuron_name
        if current:
            l += &#34; Current OFF&#34;
        labels.append(l)
        if current:
            labels.append(neuron_name+&#34; Current ON&#34;)
        ax.legend(labels)
        plt.show()
        return figure, ax

    def finalize_state_update(self, current_state, return_thresholded_potentials=False, return_dudt=False,
                              return_winners=True, n_winners=1):
        &#34;&#34;&#34;
        Generalized method to save neurons internal state after updates have been calculated, and to calculate
        the return value for the \_\_call\_\_ methods.
        Used to keep the code cleaner.
        Args:
            current_state (Tensor): Tensor of up-to-date states of neurons.
            return_thresholded_potentials (bool): Default: False.
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (int): Default 1.

        Returns:
            Tuple: Return values depends on the selected flags.
            (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;
        # inhibit where refractoriness is not consumed
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        current_state.clamp_(self.resting_potential, None)

        dudt = current_state - self.previous_state
        thresholded = self.get_thresholded_potentials(current_state)
        spiked = thresholded != 0.0

        # emitted spikes are scaled by dt
        spikes = torch.div(thresholded.sign(), self.ts)

        winners = sf.get_k_winners(thresholded, kwta=n_winners, inhibition_radius=0, spikes=spikes)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            non_inihibited_spikes[0, w[0], :, :] = True
        current_state[spiked] = self.v_reset

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        self.previous_state = current_state

        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (current_state,)
        if return_dudt:
            ret += (dudt,)
        if return_winners:
            ret += (winners,)
        return ret


class IF(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001, resting_potential=0.0, refractory_timesteps=2, C=0.281):

        &#34;&#34;&#34;
        Creates an Integrate and Fire neuron(s) that receives input potentials (from a preceding convolution)
        and updates its state according to the amount of PSP received (i.e. if it&#39;s enough, it fires a spike).
        The neuron(s) state needs to be manually reset when a sequence of related inputs ends (unless the next input is
        to be considered as related to the current one as well).

        Args:
            threshold: threshold above which the neuron(s) fires a spike
            tau_rc: the membrane time constant.
            ts: the time step used for computations, needs to be at least 10 times smaller than tau_rc.
            resting_potential: potential at which the neuron(s) is set to after a spike.
            refractory_timesteps: number of timestep of hyperpolarization after a spike.
            C: Capacitance of the membrane potential. Influences the input potential effect.
        &#34;&#34;&#34;

        # assert tau_rc / ts &gt;= 10  # needs to hold for Taylor series approximation

        super(IF, self).__init__(resting_potential=resting_potential, threshold=threshold)
        self.ts = ts
        self.tau_rc = tau_rc
        self.ts_over_tau = ts / tau_rc  # for better performance (compute once and for all)
        self.exp_term = np.exp(-self.ts_over_tau)  # for better performance (compute once and for all)
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self.refractory_periods = None
        self.C = C

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None

    def __str__(self):
        return &#34;IF_Neuron_rt&#34;+str(self.refractory_timesteps)+&#34;_C&#34;+str(self.C)

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1):
        r&#34;&#34;&#34;Computes the spike-wave tensor from tensor of potentials.
            Args:
                potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
                return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
                return_dudt (bool): Default: False.
                return_winners (bool): Default: True.
                n_winners (bool): Default: 1.
            Returns:
                Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
            &#34;&#34;&#34;

        # potentials = torch.sum(potentials, (2, 3), keepdim=True)
        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
            self.time_since_spike = torch.full(potentials.size(), 0.0, device=DEVICE)

        previous_state = self.previous_state.clone().detach()

        current_state = previous_state.float().clone().detach()


        # Input pulses.
        # In the hypothesis that dt &lt;&lt; tau_rc, we can use Taylor&#39;s expansion to approximate the exponential function.
        # In this way we can more or less simply add the potentials in.
        # input_spikes_impact = potentials * (1 - self.exp_term)

        input_spikes_impact = potentials*self.ts/self.C  # Taylor expansion form (See Neuronal Dynamics Ch.1 Â¶ 1.3.2)
        current_state += input_spikes_impact

        current_state.clip(self.resting_potential, None)

        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0
        # by using this neuron model, spikes are assumed to have amplitude $ A = A_0/t_s $ where A_0 is the spike value
        # (normally 1), and t_s is the time-step size.
        spikes = torch.div(thresholded.sign(), self.ts)
        winners = sf.get_k_winners(thresholded, spikes=spikes, kwta=n_winners)
        # name is non_inhibited_spikes because the corresponding neurons get into refractoriness as if they spiked,
        # even if they haven&#39;t actually spiked.
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            # inhibit all the feature map
            non_inihibited_spikes[0, w[0], :, :] = True

        current_state[spiked] = self.resting_potential
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        dudt = current_state - self.previous_state
        self.previous_state = current_state

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness
        self.time_since_spike += self.ts
        self.time_since_spike[spiked] = 0.0

        # emitted spikes are scaled by dt
        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (current_state,)
        if return_dudt:
            ret += (dudt,)
        if return_winners:
            ret += (winners,)
        return ret


class LIF(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001, resting_potential=0.0, refractory_timesteps=2, C=0.281,
                 per_neuron_thresh=None):

        &#34;&#34;&#34;
        Creates a Leaky Integrate and Fire neuron(s) that receives input potentials and updates its state according to
        the amount of &#39;energy&#39; received (i.e. if it&#39;s enough, it fires a spike).
        The neuron(s) state needs to be manually reset when a sequence of related inputs ends (unless the next input is
        to be considered as related to the current one as well).

        Args:
            threshold: threshold above which the neuron(s) fires a spike
            tau_rc: the membrane time constant.
            ts: the time step used for computations, needs to be at least 10 times smaller than tau_rc.
            resting_potential: potential at which the neuron(s) is set to after a spike.
            refractory_timesteps: number of timestep of hyperpolarization after a spike.
            C: Capacitance of the membrane potential. Influences the input potential effect.
            per_neuron_thresh: defines neuron-wise threshold. If None, a layer-wise threshold is used. Default: None.
        &#34;&#34;&#34;

        # assert tau_rc / ts &gt;= 10  # needs to hold for Taylor series approximation

        Neuron.__init__(self, resting_potential=resting_potential, threshold=threshold)
        self.ts = ts
        self.tau_rc = tau_rc
        self.ts_over_tau = ts / tau_rc  # for better performance (compute once and for all)
        self.exp_term = np.exp(-self.ts_over_tau)  # for better performance (compute once and for all)
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self.refractory_periods = None
        self.C = C
        self.per_neuron_thresh = per_neuron_thresh

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None

    def __str__(self):
        return &#34;LIF_Neuron_rt&#34;+str(self.refractory_timesteps)+&#34;_C&#34;+str(self.C)

    def _f_ode(self, x, I=0):
        return -(x - self.resting_potential) + (self.tau_rc/self.C)*I

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1, return_winning_spikes=False):
        r&#34;&#34;&#34;Computes a (time-) step update for layer of LIF neurons.

            Args:
                potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
                return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
                return_dudt (bool): Default: False.
                return_winners (bool): Default: True.
                n_winners (bool): Default: 1.
            Returns:
                Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
            &#34;&#34;&#34;
        # potentials = torch.sum(potentials, (2, 3), keepdim=True)
        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
            self.time_since_spike = torch.full(potentials.size(), 0.0, device=DEVICE)
            self.time_since_injection = torch.full(potentials.size(), 0.0, device=DEVICE)
            self.potential_at_last_injection = torch.full(potentials.size(), self.resting_potential, device=DEVICE)

        previous_state = self.previous_state.clone().detach()

        # ## adapted from Nengo LIF neuron ## #

        # Exponential decay of the membrane potential.
        # To avoid the need of an extra tensor of time-since-last-spike, we can model it as a difference using
        # a constant exponential time step for the decay and eventually clipping the value to resting_pot.
        # pot = resting_pot + torch.mul((previous_state - resting_pot), np.exp(-dt/tau_rc))
        exp_term = torch.clip(torch.exp(-self.time_since_injection/self.tau_rc), max=1)
        current_state = self.resting_potential + (self.potential_at_last_injection - self.resting_potential) * exp_term

        # Input pulses.
        # In the hypothesis that dt &lt;&lt; tau_rc (at least one order of magnitude), we can use Taylor&#39;s expansion
        # to approximate the exponential function. In this way we can more or less simply add the potentials in.
        # input_spikes_impact = potentials * (1 - self.exp_term)

        input_spikes_impact = potentials*self.ts/self.C  # Taylor expansion form (See Neuronal Dynamics Ch.1 Â¶ 1.3.2)
        current_state += input_spikes_impact
        self.time_since_injection += self.ts
        self.time_since_injection[input_spikes_impact &gt; 0] = self.ts
        # inhibit where refractory period is not yet passed.
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        current_state.clip(self.resting_potential, None)
        self.potential_at_last_injection[input_spikes_impact &gt; 0] = current_state[input_spikes_impact &gt; 0]
        dudt = current_state - self.previous_state
        # resting = torch.full(potentials.size(), resting_pot)
        # delta = torch.add(potentials, -previous_state)
        # delta = torch.add(-previous_state, resting_pot)
        # exp_term = -np.expm1(-dt / leaky_term)
        # delta = torch.mul(delta, exp_term)
        # current_state = torch.add(previous_state, -delta)
        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0
        # by using this neuron model, spikes are assumed to have amplitude $ A = A_0/t_s $ where A_0 is the spike value
        # (here 1), and t_s is the time-step size.
        spikes = torch.div(thresholded.sign(), self.ts)

        winners = sf.get_k_winners(thresholded, spikes=spikes, kwta=n_winners)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            if self.inhibition_mode == &#34;feature&#34;:  # inhibit all the feature map
                non_inihibited_spikes[0, w[0], :, :] = True  # This is then used to inhibit all neurons in the same feature-group of neurons as the one who winned
            elif self.inhibition_mode == &#34;location&#34;:
                non_inihibited_spikes[0, :, w[1], w[2]] = True
            # non_inihibited_spikes[0] = True
        current_state[spiked] = self.resting_potential
        self.previous_state = current_state

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness
        self.time_since_spike += self.ts
        self.time_since_spike[spiked] = 0.0

        # emitted spikes are scaled by dt
        ret = (spikes, )
        if return_thresholded_potentials:
            ret += (thresholded, )
        ret += (current_state, )
        if return_dudt:
            ret += (dudt, )
        if return_winners:
            ret += (winners, )
        if return_winning_spikes:
            not_winning_spikes = torch.full(spiked.shape, True)
            for w in winners:
                not_winning_spikes[0, w[0], w[1], w[2]] = False
            ws = spikes.clone()
            ws[not_winning_spikes] = 0.0
            ret += (ws, )
        return ret

class LIF_Simple(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001, resting_potential=0.0, refractory_timesteps=2,
                 per_neuron_threshold=None):
        &#34;&#34;&#34;
            A simplified version of the LIF neuron which does not take into account the capacitance and uses a simple decay.
            With this class, spikes are propagated with amplitude \(A = 1\), instead of \(A = \\frac{1}{t_s}\)

            Args:
                threshold: threshold above which the neuron(s) fires a spike
                tau_rc: the membrane time constant.
                ts: the time step used for computations, needs to be at least 10 times smaller than tau_rc.
                resting_potential: potential at which the neuron(s) is set to after a spike.
                refractory_timesteps: number of timestep of hyperpolarization after a spike.
                C: Capacitance of the membrane potential. Influences the input potential effect.
                per_neuron_thresh: Defines neuron-wise threshold. If None, a layer-wise threshold is used. Default: None.
        &#34;&#34;&#34;
        Neuron.__init__(self, resting_potential=resting_potential, threshold=threshold)
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = refractory_timesteps*ts
        self.per_neuron_thresh = per_neuron_threshold
        self.tau_rc = tau_rc
        self.ts = ts
        assert tau_rc &gt; 3*ts  # needed for Taylor approx.; actually would be better with 6 times more than ts
        self.decay = 1 - ts/tau_rc  # Taylor approx

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None

    def __str__(self):
        return &#34;LIF_Simple_RT&#34; + str(self.refractory_timesteps) + &#34;_tau&#34; + str(self.tau_rc)

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1, return_winning_spikes=False):
        &#34;&#34;&#34;
        Calculates a (time-) step update for the layer of LIF neurons.

        Args:
            potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
            return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (bool): Default: 1.
        Returns:
            Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;
        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
        previous_state = self.previous_state.clone().detach()

        current_state = previous_state*self.decay
        current_state += potentials
        current_state[self.refractory_periods &gt; 0] = self.resting_potential

        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0
        spikes = thresholded.sign()

        winners = sf.get_k_winners(thresholded, spikes=spikes, kwta=n_winners)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            if self.inhibition_mode == &#34;feature&#34;:  # inhibit all the feature map
                non_inihibited_spikes[0, w[0], :, :] = True  # This is then used to inhibit all neurons in the same feature-group of neurons as the one who winned
            elif self.inhibition_mode == &#34;location&#34;:
                non_inihibited_spikes[0, :, w[1], w[2]] = True
            # non_inihibited_spikes[0] = True  # to be used in single-neuron scenarios
        current_state[spiked] = self.resting_potential
        self.previous_state = current_state

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        # emitted spikes are NOT scaled by dt
        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (current_state,)
        if return_dudt:
            ret += (current_state-previous_state,)
        if return_winners:
            ret += (winners,)
        if return_winning_spikes:
            not_winning_spikes = torch.full(spiked.shape, True)
            for w in winners:
                not_winning_spikes[0, w[0], w[1], w[2]] = False
            ws = spikes.clone()
            ws[not_winning_spikes] = 0.0
            ret += (ws,)
        return ret

class LIF_ode(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001, resting_potential=0.0, refractory_timesteps=2, C=0.281,
                 per_neuron_thresh=None):

        &#34;&#34;&#34;
        Creates a Leaky Integrate and Fire neuron(s) that receives input potentials and updates its state according to the amount of &#39;energy&#39; received (i.e. if it&#39;s enough, it fires a spike).
        Differently from the LIF class, the LIF_ode uses the LIF ode to directly calculate updates time-step by time-step.


        Args:
                threshold: threshold above which the neuron(s) fires a spike
                tau_rc: the membrane time constant.
                ts: the time step used for computations, needs to be at least 10 times smaller than tau_rc.
                resting_potential: potential at which the neuron(s) is set to after a spike.
                refractory_timesteps: number of timestep of hyperpolarization after a spike.
                C: Capacitance of the membrane potential. Influences the input potential effect.
                per_neuron_thresh: Defines neuron-wise threshold. If None, a layer-wise threshold is used. Default: None.
        &#34;&#34;&#34;

        # assert tau_rc / ts &gt;= 10  # needs to hold for Taylor series approximation

        super(LIF_ode, self).__init__(resting_potential=resting_potential, threshold=threshold)
        self.ts = ts
        self.tau_rc = tau_rc
        self.ts_over_tau = ts / tau_rc  # for better performance (compute once and for all)
        self.exp_term = np.exp(-self.ts_over_tau)  # for better performance (compute once and for all)
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self.refractory_periods = None
        self.C = C
        self.per_neuron_thresh = per_neuron_thresh

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None

    def __str__(self):
        return &#34;LIF_ode_Neuron_rt&#34;+str(self.refractory_timesteps)+&#34;_C&#34;+str(self.C)

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1):
        &#34;&#34;&#34;Computes a (time-) step update for the layer of LIF neurons.
            Args:
                potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
                return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
                return_dudt (bool): Default: False.
                return_winners (bool): Default: True.
                n_winners (bool): Default: 1.
            Returns:
                Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
            &#34;&#34;&#34;
        # potentials = torch.sum(potentials, (2, 3), keepdim=True)
        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)

        previous_state = self.previous_state.clone().detach()

        du = (self.resting_potential - previous_state)*self.ts_over_tau
        du += potentials * self.ts / self.C

        current_state = previous_state + du

        # inhibit where refractory period is not yet passed.
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        current_state.clip(self.resting_potential, None)

        dudt = current_state - self.previous_state

        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0
        spikes = torch.div(thresholded.sign(), self.ts)

        winners = sf.get_k_winners(thresholded, spikes=spikes, kwta=n_winners)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            # inhibit the whole feature map for the next iteration. One feature map = one feature.
            # One neuron firing in one feature map = one feature at that position.
            non_inihibited_spikes[0, w[0], :, :] = True
            # non_inihibited_spikes[0] = True
        current_state[spiked] = self.resting_potential

        self.previous_state = current_state

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        # emitted spikes are scaled by dt
        ret = (spikes, )
        if return_thresholded_potentials:
            ret += (thresholded, )
        ret += (current_state, )
        if return_dudt:
            ret += (dudt, )
        if return_winners:
            ret += (winners, )
        return ret


class EIF(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001, delta_t=0.5, theta_rh=None, resting_potential=0.0,
                 refractory_timesteps=2, C=0.281, v_reset=None):
        &#34;&#34;&#34;
        Creates a layer of exponential integrate and fire neurons.
        Args:
            threshold: Default: None.
            tau_rc: Membrane time constant a.k.a. tau_m or tau in seconds. Default: 0.02.
            ts: time-step value in seconds. Default: 0.001.
            delta_t: Sharpness parameter (upswing on the exponential curve). If ~0, EIF behaves like LIF. Default: 0.5.
            theta_rh: Rheobase threshold. Default: None.
            resting_potential: Default: 0.0.
            refractory_timesteps: Default: 2.
            C: Capacitance. Default: 0.281.
            v_reset: Default: None.


        .. note:: `theta_rh` being `None` will cause `theta_rh` to be \(\\frac{3}{4}V_{thresh}\).
        &#34;&#34;&#34;
        Neuron.__init__(self, resting_potential=resting_potential, threshold=threshold)

        # assert abs(theta_rh / (resting_potential + delta_t)) &gt;= 10, \
        #     &#34;Needs to hold as it is assumed in Neuronal Dynamics book, Ch.5 Â¶ 5.2&#34;
        self.tau_rc = tau_rc
        self.ts = ts
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self.refractory_periods = None
        self.C = C
        self.delta_t = delta_t
        if theta_rh is None:  # guess from threshold
            theta_rh = -abs(threshold)*0.25 + threshold  # make the rheobase threshold smaller of the threshold by 25%
        self.theta_rh = theta_rh
        if v_reset is None:
            self.v_reset = resting_potential
        else:
            self.v_reset = v_reset

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None

    def __str__(self):
        return &#34;EIF_Neuron_rt&#34;+str(self.refractory_timesteps)+&#34;_C&#34;+str(self.C)

    def _f_ode(self, x, I=0):
        return -(x - self.resting_potential) + self.delta_t*np.exp((x-self.theta_rh)/self.delta_t) + (self.tau_rc / self.C) * I

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1):
        &#34;&#34;&#34;
        Calculates the (time-) step update for the neurons as specified by the following differential equation:
        $$
            \\tau_{rc}\\frac{du}{dt} = -(u - u_{rest}) + \\Delta_T \\cdot \\exp\\left({\\frac{u - \\Theta_{rh}}{\\Delta_T}}\\right)
            + R\\cdot I(t)
        $$
        Args:
            potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
            return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (bool): Default: 1.
        Returns:
            Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;
        # potentials = torch.sum(potentials, (2, 3), keepdim=True)

        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)

        previous_state = self.previous_state.clone().detach()

        du = self.resting_potential - previous_state  # -(previous_state - self.resting_potential)
        du = du + self.delta_t * torch.exp((previous_state - self.theta_rh) / self.delta_t)
        du /= self.tau_rc
        du += potentials/self.C
        du *= self.ts
        current_state = previous_state + du

        # inhibit where refractoriness is not consumed
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        current_state.clamp_(self.resting_potential, None)

        dudt = current_state - self.previous_state
        # current_state.clip(self.resting_potential, None)

        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0

        # emitted spikes are scaled by dt
        spikes = torch.div(thresholded.sign(), self.ts)

        winners = sf.get_k_winners(thresholded, kwta=n_winners, inhibition_radius=0, spikes=spikes)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            non_inihibited_spikes[0, w[0], :, :] = True
        current_state[spiked] = self.v_reset

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        self.previous_state = current_state

        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (current_state,)
        if return_dudt:
            ret += (dudt,)
        if return_winners:
            ret += (winners,)
        return ret


class EIF_Simple(Neuron):
    def __init__(self, threshold, tau_rc=0.02, ts=0.001, delta_t=0.5, theta_rh=None, resting_potential=0.0,
                 refractory_timesteps=2, v_reset=None, per_neuron_threshold=None):
        &#34;&#34;&#34;
        Creates a layer of exponential integrate and fire neurons. These neurons are simplified with respect to the EIF class, in the sense that the capacitance is not used anymore, the linear decay is implemented through a simple multiplication and the incoming potentials are not expected to be scaled by \(\\frac{1}{t_s}\).
        Args:
            threshold: Default: None.
            tau_rc: Membrane time constant a.k.a. tau_m or tau in seconds. Default: 0.02.
            ts: time-step value in seconds. Default: 0.001.
            delta_t: Sharpness parameter (upswing on the exponential curve). If ~0, EIF behaves like LIF. Default: 0.5.
            theta_rh: Rheobase threshold. Default: None.
            resting_potential: Default: 0.0.
            refractory_timesteps: Default: 2.
            C: Capacitance. Default: 0.281.
            v_reset: Default: None.

        .. note:: `theta_rh` being `None` will cause `theta_rh` to be \(\\frac{3}{4}V_{thresh}\).
        &#34;&#34;&#34;
        Neuron.__init__(self, resting_potential=resting_potential, threshold=threshold)
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self._per_neuron_thresh = per_neuron_threshold
        self.tau_rc = tau_rc
        self.ts = ts
        assert tau_rc &gt; 3*ts  # needed for Taylor approx.; actually would be better with 6 times more than ts
        self.decay = 1 - ts/tau_rc  # Taylor approx

        self.delta_t = delta_t
        if theta_rh is None:  # guess from threshold
            theta_rh = -abs(threshold)*0.25 + threshold  # make the rheobase threshold smaller of the threshold by 25%
        self.theta_rh = theta_rh
        if v_reset is None:
            self.v_reset = resting_potential
        else:
            self.v_reset = v_reset

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None

    @Neuron.per_neuron_thresh.setter
    def per_neuron_thresh(self, value):
        self._per_neuron_thresh = value
        self.theta_rh = 0.75*value

    def __str__(self):
        return &#34;EIF_Simple_RT&#34; + str(self.refractory_timesteps) + &#34;_tau&#34; + str(self.tau_rc)

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1, return_winning_spikes=False):
        &#34;&#34;&#34;
        Calculates the (time-) step update for the neurons as specified by the following differential equation:
        $$
            \\tau_{rc}\\frac{du}{dt} = -(u - u_{rest}) + \\Delta_T \\cdot \\exp\\left({\\frac{u - \\Theta_{rh}}{\\Delta_T}}\\right)
            + R\\cdot I(t)
        $$
        Args:
            potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
            return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (bool): Default: 1.
        Returns:
            Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;
        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
        previous_state = self.previous_state.clone().detach()

        current_state = previous_state*self.decay + self.delta_t * torch.exp((previous_state - self.theta_rh) / self.delta_t)
        current_state += potentials
        current_state[self.refractory_periods &gt; 0] = self.resting_potential

        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0
        spikes = thresholded.sign()

        winners = sf.get_k_winners(thresholded, spikes=spikes, kwta=n_winners)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            if self.inhibition_mode == &#34;feature&#34;:  # inhibit all the feature map
                non_inihibited_spikes[0, w[0], :, :] = True  # This is then used to inhibit all neurons in the same feature-group of neurons as the one who winned
            elif self.inhibition_mode == &#34;location&#34;:
                non_inihibited_spikes[0, :, w[1], w[2]] = True
            # non_inihibited_spikes[0] = True
        # neurons that fired a spike a reset to v_reset regardless of being winners
        membrane_potential = current_state.clone().detach()
        current_state[spiked] = self.v_reset
        self.previous_state = current_state

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        # but only the winners (and the inhibited neurons) are given a refractory period.
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        # emitted spikes are NOT scaled by dt
        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (membrane_potential,)  # (current_state,)
        if return_dudt:
            ret += (current_state-previous_state,)  # this will result in very negative dudt sometimes, can be done better
        if return_winners:
            ret += (winners,)
        if return_winning_spikes:
            not_winning_spikes = torch.full(spiked.shape, True)
            for w in winners:
                not_winning_spikes[0, w[0], w[1], w[2]] = False
            ws = spikes.clone()
            ws[not_winning_spikes] = 0.0
            ret += (ws,)
        return ret

class AdEx(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001, delta_t=0.5,
                 theta_rh=None, resting_potential=0.0,
                 refractory_timesteps=2, C=0.281, v_reset=None,
                 a=0.6, b=0.7, tau_w=1):
        &#34;&#34;&#34;
        Creates a layer of Adaptive Exponential Integrate and Fire (AdEx) neurons.
        Args:
            threshold: Default: None.
            tau_rc: Membrane time constant a.k.a. tau_m or tau, in seconds. Default: 0.02.
            ts: time-step value in seconds. Default: 0.001.
            delta_t: Sharpness parameter (upswing on the exponential curve). If ~0, EIF behaves like LIF. Default: 0.5.
            theta_rh: Rheobase threshold, if None it&#39;s equal to \(\\frac{3}{4}V_{thresh}\). Default: None.
            resting_potential: Default: 0.0.
            refractory_timesteps: Default: 2.
            C: Capacitance. Default: 0.281.
            v_reset: After-spike reset voltage, if None it is equal to the resting potential. Default: None.
            a: Adaptation variable parameter to regulate the adaptation dependence from the membrane potential. Default: 0.6.
            b: Adaptation variable parameter to regulate the adaptation increase upon emission of a spike. Default: 0.7.
            tau_w: Adaptation variable time constant. Default: 1.
        &#34;&#34;&#34;
        super(AdEx, self).__init__(resting_potential=resting_potential, threshold=threshold)

        # assert abs(theta_rh / (resting_potential + delta_t)) &gt;= 10, \
        #     &#34;Needs to hold as it is assumed in Neuronal Dynamics book, Ch.5 Â¶ 5.2&#34;
        self.tau_rc = tau_rc
        self.ts = ts
        self.delta_t = delta_t
        if theta_rh is None:  # guess from threshold
            theta_rh = -abs(threshold)*0.25 + threshold  # make the rheobase threshold smaller of the threshold by 25%
        self.theta_rh = theta_rh
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self.refractory_periods = None
        self.C = C
        self.R = tau_rc/C
        self.a = a
        self.b = b
        self.tau_w = tau_w
        if v_reset is None:
            self.v_reset = resting_potential
        else:
            self.v_reset = v_reset

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None
        self.w = None

    def __str__(self):
        return &#34;AdEx_Neuron_rt&#34;+str(self.refractory_timesteps)+&#34;_C&#34;+str(self.C)

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1):
        &#34;&#34;&#34;
        Calculates a (time-) step update for the neuron(s) as specified by the following differential equations:
        $$
            \\tau_{rc}\\frac{du}{dt} = -(u - u_{rest}) + \\Delta_T \\cdot \\exp\\left({\\frac{u - \\Theta_{rh}}{\\Delta_T}}\\right)
            - R\\cdot \\omega + R\\cdot I(t) \\\\
            \\tau_w\\frac{d\\omega}{dt} = a(u - u_{rest}) + b\\sum_{t^{(f)}}\\delta(t-t^{(f)})
        $$
        Args:
            potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
            return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (bool): Default: 1.
        Returns:
            Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;
        # potentials = torch.sum(potentials, (2, 3), keepdim=True)

        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
            self.w = torch.full(potentials.size(), 0.0, device=DEVICE)

        previous_state = self.previous_state.clone().detach()

        du = self.resting_potential - previous_state  # -(previous_state - self.resting_potential)
        du = du + self.delta_t * torch.exp((previous_state - self.theta_rh) / self.delta_t)
        currents_impact = (potentials - self.w)*self.R
        du += currents_impact
        du /= self.tau_rc
        du *= self.ts
        current_state = previous_state + du

        # inhibit where refractoriness is not consumed
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        current_state.clamp_(self.resting_potential, None)

        dudt = current_state - self.previous_state
        # current_state.clip(self.resting_potential, None)

        # TODO: Maybe, given that a proper assumption according to Neuronal Dynamics book, would be to have
        # TODO: threshold &gt;&gt; theta_rh + delta_t, if it is None I could set it to be 1 order of magnitude greater?
        # TODO: i.e. threshold = (delta_T + theta_rh) * 10
        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0

        # Implement the common adaptation variable update for all the neurons
        self.w += (self.a*(current_state-self.resting_potential) - self.w)/self.tau_w*self.ts
        # Add a current jump only where there has been a spike
        self.w[spiked] += self.b  # see https://journals.physiology.org/doi/pdf/10.1152/jn.00686.2005 for
        #                         # why this is simply added in.

        # emitted spikes are scaled by dt
        spikes = torch.div(torch.abs(thresholded.sign()), self.ts)

        winners = sf.get_k_winners(thresholded, kwta=n_winners, inhibition_radius=0, spikes=spikes)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            # non_inihibited_spikes[0, w[0], :, :] = True
            non_inihibited_spikes[0] = True
        current_state[spiked] = self.v_reset

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        self.previous_state = current_state

        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (current_state,)
        if return_dudt:
            ret += (dudt,)
        if return_winners:
            ret += (winners,)
        return ret


class QIF(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001, u_c=None, a=0.001, resting_potential=0.0,
                 refractory_timesteps=2, C=0.281, v_reset=None):
        &#34;&#34;&#34;
        Creates a layer of Quadratic Integrate-and-Fire (QIF) neurons.
        Args:
            threshold: Default: None.
            tau_rc: Membrane time constant a.k.a. tau_m or tau in seconds. Default: 0.02.
            ts: time-step value in seconds. Default: 0.001.
            Cut-off threshold (negative-positive membrane potential update transition point). Default: None.
            a: Sharpness parameter (upswing on the parabolic curve). Default: None.
            resting_potential: Default: 0.0.
            refractory_timesteps: Default: 2.
            C: Capacitance. Default: 0.281.
            v_reset: Default: None.


        .. note:: `u_c` being `None` will cause `u_c` to be \(\\frac{3}{4}V_{thresh}\).
        &#34;&#34;&#34;
        Neuron.__init__(self, resting_potential=resting_potential, threshold=threshold)

        # assert abs(theta_rh / (resting_potential + delta_t)) &gt;= 10, \
        #     &#34;Needs to hold as it is assumed in Neuronal Dynamics book, Ch.5 Â¶ 5.2&#34;
        self.tau_rc = tau_rc
        self.ts = ts
        if u_c is None:  # guess from threshold
            u_c = -abs(threshold)*0.25 + threshold  # make the &#39;rheobase&#39; threshold smaller of the threshold by 25%
        self.u_c = u_c
        self.a = a
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self.refractory_periods = None
        self.C = C
        if v_reset is None:
            self.v_reset = resting_potential
        else:
            self.v_reset = v_reset

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None

    def __str__(self):
        return &#34;QIF_Neuron_rt&#34;+str(self.refractory_timesteps)+&#34;_C&#34;+str(self.C)

    def _f_ode(self, x, I=0):
        return self.a*(x - self.resting_potential)*(x-self.u_c) + (self.tau_rc/self.C)*I

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1):
        &#34;&#34;&#34;
        Calculates a (time-) step update for the neuron as specified by the following differential equation:
        $$
            \\tau_{rc}\\frac{du}{dt} = -a_0(u - u_{rest})(u - u_{c}) + R\\cdot I(t)
        $$
        Args:
            potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
            return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (bool): Default: 1.
        Returns:
            Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;

        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)

        previous_state = self.previous_state.clone().detach()

        # TODO should consider multiplying input by a resistance?
        # du = -(previous_state - self.resting_potential) \
        #      + self.delta_t * torch.exp((previous_state - self.theta_rh) / self.delta_t) \
        #      + potentials

        du = previous_state - self.resting_potential
        du = du * (previous_state - self.u_c) * self.a
        du /= self.tau_rc
        du += potentials/self.C
        du *= self.ts
        current_state = previous_state + du

        # inhibit where refractoriness is not consumed
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        torch.clip_(current_state, min=self.resting_potential)
        dudt = current_state - self.previous_state

        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0
        spikes = torch.div(thresholded.sign(), self.ts)
        # winners = sf.get_k_winners_davide(thresholded, spikes, self.per_neuron_thresh)
        winners = sf.get_k_winners(thresholded, kwta=n_winners, inhibition_radius=0, spikes=spikes)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            non_inihibited_spikes[0, w[0], :, :] = True
        current_state[spiked] = self.v_reset

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        self.previous_state = current_state

        # emitted spikes are scaled by dt
        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (current_state,)
        if return_dudt:
            ret += (dudt,)
        if return_winners:
            ret += (winners,)
        return ret


class Izhikevich(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001,
                 a=0.02, b=0.2, c=0.0, d=8.0,
                 resting_potential=0.0,
                 refractory_timesteps=2, C=0.281, v_reset=None):
        &#34;&#34;&#34;
        Creates a layer of Izhikevich&#39;s neurons.

        Args:
            tau_rc: Membrane time constant a.k.a. tau_m or tau in seconds. Default: 0.02.
            ts: time-step value in seconds. Default: 0.001.
            delta_t: Sharpness parameter (upswing on the exponential curve). If ~0, EIF behaves like LIF. Default: 0.5.
            theta_rh: Rheobase threshold. Default: 5.
            resting_potential: Default: 0.0.
            threshold: Default: None.
        &#34;&#34;&#34;
        super(Izhikevich, self).__init__(resting_potential=resting_potential, threshold=threshold)

        # assert abs(theta_rh / (resting_potential + delta_t)) &gt;= 10, \
        #     &#34;Needs to hold as it is assumed in Neuronal Dynamics book, Ch.5 Â¶ 5.2&#34;
        self.tau_rc = tau_rc
        self.ts = ts
        # if theta_rh is None:  # guess from threshold
        #     theta_rh = -abs(threshold)*0.25 + threshold  # make the rheobase threshold smaller of the threshold by 25%
        # self.theta_rh = theta_rh
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self.refractory_periods = None
        self.C = C
        self.R = tau_rc/C
        self.a = a
        self.b = b
        self.d = d
        if v_reset is None:
            self.v_reset = resting_potential
        else:
            self.v_reset = v_reset
        self.c = c if c is not None else self.v_reset

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None
        self.w = None

    def __str__(self):
        return &#34;Izhikevich_Neuron_rt&#34;+str(self.refractory_timesteps)+&#34;_C&#34;+str(self.C)

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1):
        &#34;&#34;&#34;
        Calculates the update amount for the neuron as specified by the following differential equations:

        $$
            \\frac{du}{dt} = 0.04u^2 + 5u + 140 - \\omega + I \\\\
            \\frac{d\\omega}{dt} = a(b\\cdot u - \\omega)
        $$
        Args:
            potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
            return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (bool): Default: 1.
        Returns:
            Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;
        # potentials = torch.sum(potentials, (2, 3), keepdim=True)

        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
            self.w = torch.full(potentials.size(), self.resting_potential*self.b, device=DEVICE)

        previous_state = self.previous_state.clone().detach()

        du = 0.04*previous_state**2 + 5*previous_state + 140
        du += potentials
        du -= self.w
        current_state = previous_state + du

        # inhibit where refractoriness is not consumed
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        # current_state.clamp_(self.resting_potential, None)

        dw = self.a*(self.b*current_state - self.w)
        dudt = current_state - self.previous_state
        # current_state.clip(self.resting_potential, None)

        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0
        # Add a current jump only where there has been a spike
        self.w = self.w + dw
        self.w[spiked] += self.d

        # emitted spikes are scaled by dt
        spikes = torch.div(torch.abs(thresholded.sign()), self.ts)

        winners = sf.get_k_winners(thresholded, kwta=n_winners, inhibition_radius=0, spikes=spikes)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            #non_inihibited_spikes[0, w[0], :, :] = True
            non_inihibited_spikes[0] = True  # TODO
        current_state[spiked] = self.v_reset

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        self.previous_state = current_state

        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (current_state,)
        if return_dudt:
            ret += (dudt,)
        if return_winners:
            ret += (winners,)
        return ret


class HeterogeneousNeuron(Neuron):

    def __init__(self, conv):
        &#34;&#34;&#34;
        Base class for layers of neurons having a non-homogeneous set of parameters.
        &#34;&#34;&#34;
        super().__init__(conv)

    def get_uniform_distribution(self, range, size):
        &#34;&#34;&#34;
        Creates a uniformly distributed set of values in the `range` and `size` provided.
        Args:
            range (list): Range to sample the values from.
            size (tuple): Size of the Tensor to sample.

        Returns:
            Tensor: Tensor containing the uniformly distributed values.
        &#34;&#34;&#34;
        ones = np.ones(size)
        uniform = np.random.uniform(*range, size=(size[0], size[1], 1, 1))
        uniform = torch.from_numpy(uniform*ones)
        return uniform.to(DEVICE)


class UniformLIF(LIF, HeterogeneousNeuron):

    def __init__(self, threshold, tau_range, ts=0.001, resting_potential=0.0, refractory_timesteps=2, C=0.281,
                 per_neuron_thresh=None):
        &#34;&#34;&#34;
        Creates a layer of heterogeneous Leaky Integrate and Fire neuron(s).

        Args:
            threshold: threshold above which the neuron(s) fires a spike.
            tau_range (list): Range of values from which to sample the \(\\tau_{rc}\).
            ts: the time step used for computations, needs to be at least 10 times smaller than tau_rc.
            resting_potential: potential at which the neuron(s) is set to after a spike.
            refractory_timesteps: number of timestep of hyperpolarization after a spike.
            C: Capacitance of the membrane potential. Influences the input potential effect.
            per_neuron_thresh: defines neuron-wise threshold. If None, a layer-wise threshold is used. Default: None.
        &#34;&#34;&#34;
        LIF.__init__(self, threshold, C=C, refractory_timesteps=refractory_timesteps, ts=ts,
                     resting_potential=resting_potential, per_neuron_thresh=per_neuron_thresh)
        self.threshold = threshold
        self.tau_range = tau_range
        self.taus = None

    def __call__(self, potentials, *args, **kwargs):

        if self.taus is None:
            self.taus = self.get_uniform_distribution(self.tau_range, potentials.shape)
            self.ts_over_tau = self.ts / self.taus.cpu().numpy()  # for better performance (compute once and for all)
            self.exp_term = np.exp(-self.ts_over_tau)  # for better performance (compute once and for all)

        return super(UniformLIF, self).__call__(potentials, *args, **kwargs)


class UniformEIF(EIF, HeterogeneousNeuron):

    def __init__(self, threshold, tau_range, ts=0.001, delta_t=0.5, theta_rh=None, resting_potential=0.0,
                 refractory_timesteps=2, C=0.281, v_reset=None):
        &#34;&#34;&#34;
        Creates a layer of heterogeneous Exponential Integrate and Fire (EIF) neurons.

        Args:
            threshold: Default: None.
            tau_range (list): Range of values from which to sample the \(\\tau_{rc}\).
            ts: time-step value in seconds. Default: 0.001.
            delta_t: Sharpness parameter (upswing on the exponential curve). If ~0, EIF behaves like LIF. Default: 0.5.
            theta_rh: Rheobase threshold. Default: None.
            resting_potential: Default: 0.0.
            refractory_timesteps: Default: 2.
            C: Capacitance. Default: 0.281.
            v_reset: Default: None.

        .. note:: `theta_rh` being `None` will cause `theta_rh` to be \(\\frac{3}{4}V_{thresh}\).
        &#34;&#34;&#34;
        EIF.__init__(self, threshold=threshold, tau_rc=0.02, ts=ts, delta_t=delta_t, theta_rh=theta_rh,
                     resting_potential=resting_potential, refractory_timesteps=refractory_timesteps, C=C,
                     v_reset=v_reset)

        HeterogeneousNeuron.__init__(self)
        self.tau_range = tau_range
        self.taus = None

    def __call__(self, potentials, *args, **kwargs):

        if self.taus is None:
            self.taus = self.get_uniform_distribution(self.tau_range, potentials.shape)

        return super(UniformEIF, self).__call__(potentials, *args, **kwargs)


class UniformQIF(QIF, HeterogeneousNeuron):

    def __init__(self, threshold, tau_range, ts=0.001, u_c=None, a=0.001, resting_potential=0.0,
                 refractory_timesteps=2, C=0.281, v_reset=None):
        &#34;&#34;&#34;
        Creates a layer of heterogeneous Quadratic Integrate-and-Fire (QIF) neurons.
        Args:
            threshold: Default: None.
            tau_range (list): Range of values from which to sample the \(\\tau_{rc}\).
            ts: time-step value in seconds. Default: 0.001.
            u_c: Cut-off threshold (negative-positive membrane potential update transition point). Default: 5.
            a: Sharpness parameter (upswing on the parabolic curve). Default: None.
            resting_potential: Default: 0.0.
            refractory_timesteps: Default: 2.
            C: Capacitance. Default: 0.281.
            v_reset: Default: None.

        .. note:: `u_c` being `None` will cause `u_c` to be \(\\frac{3}{4}V_{thresh}\).
        &#34;&#34;&#34;
        QIF.__init__(self, threshold, tau_rc=0.02, ts=ts, u_c=u_c, a=a, resting_potential=resting_potential,
                 refractory_timesteps=refractory_timesteps, C=C, v_reset=v_reset)
        HeterogeneousNeuron.__init__(self)
        self.tau_range = tau_range
        self.taus = None

    def __call__(self, potentials, *args, **kwargs):
        if self.taus is None:
            self.taus = self.get_uniform_distribution(self.tau_range, potentials.shape)

        return super(UniformQIF, self).__call__(potentials, *args, **kwargs)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="SpykeTorch.neurons.Neuron"><code class="flex name class">
<span>class <span class="ident">Neuron</span></span>
<span>(</span><span>ts=0.01, resting_potential=0.0, v_reset=None, threshold=None, refractory_timesteps=0, inhibition_mode='feature')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Neuron(object):

    def __init__(self, ts=0.01, resting_potential=0.0, v_reset=None, threshold=None, refractory_timesteps=0,
                 inhibition_mode=&#34;feature&#34;):
        self._threshold = threshold

        self.resting_potential = resting_potential
        self.previous_state = None
        self._per_neuron_thresh = None
        self.refractory_timestes = refractory_timesteps
        self.ts = ts
        self.refractoriness = refractory_timesteps*self.ts
        self.refractory_periods = None
        self.v_reset = v_reset if v_reset is not None else resting_potential
        self.inhibition_mode = inhibition_mode

    @property
    def threshold(self):
        return self._threshold

    @threshold.setter
    def threshold(self, threshold):
        self._threshold = threshold

    @property
    def per_neuron_thresh(self):
        return self._per_neuron_thresh

    @per_neuron_thresh.setter
    def per_neuron_thresh(self, value):
        self._per_neuron_thresh = value

    def __str__(self):
        return &#34;Spiking_Neuron_Base_Class&#34;

    def __call__(self, *args, **kwargs):
        raise NotImplementedError()

    def reset(self):
        raise NotImplementedError()

    def get_params(self):
        params = {}
        for k, v in self.__dict__.items():
            if not isinstance(v, torch.Tensor):
                params[k] = v
        return params

    def get_thresholded_potentials(self, current_state):
        &#34;&#34;&#34;
        General method to get thresholded membrane potentials, i.e. a Tensor where values are != 0.0 only if they were above
        the corresponding neuron&#39;s threshold.
        Args:
            current_state (Tensor): membrane potentials of the neurons

        Returns:
            Tensor: thresholded membrane potentials.
        &#34;&#34;&#34;
        thresholded = current_state.clone().detach()

        # inhibit where refractoriness is not consumed
        thresholded[self.refractory_periods &gt; 0] = self.resting_potential

        if self.per_neuron_thresh is None:
            self.per_neuron_thresh = torch.ones(current_state.shape, device=DEVICE)*self.threshold
        if self._threshold is None:
            thresholded[:-1] = 0
        else:
            thresholded[thresholded &lt; self.per_neuron_thresh] = 0.0

        return thresholded

    def _f_ode(self, x, I=0):
        raise NotImplementedError()

    def plot_ode(self, figure: plt.Figure = None, ax: plt.Axes = None, current=None):
        &#34;&#34;&#34;
        Plots the neuron&#39;s ODE. If current is given, ODE with and without current are plotted.
        Multiple plots can be stack onto each other by passing the proper figure and axes as an argument.
        Args:
            figure (pyplot.Figure): Figure to use for plots. If None, a new one is created.
            ax (pyplot.Axes): Axes to use for the plot. If None, a new one is created.
            current: If provided, draws the current ON/OFF plots.

        Returns:
            Tuple: The figure and axes of the plot.
        &#34;&#34;&#34;
        f = False
        if figure is None:
            figure = plt.figure()
            f = True
        if ax is None:
            ax = figure.add_subplot()  # type: plt.Axes

        neuron_name = self.__str__().split(&#34;_&#34;)[0]
        x_points = np.linspace(self.resting_potential, self.threshold + self.threshold / 2, 300)
        y0 = np.array([self._f_ode(x, 0) for x in x_points])
        ax.plot(x_points, y0)
        if current:
            y1 = np.array([self._f_ode(x, current) for x in x_points])
            ax.plot(x_points, y1)
        xmax = ax.get_xlim()[1]
        ax.set_xlim(min(x_points), max(max(x_points), xmax))
        ymin, ymax = ax.get_ylim()
        if f:
            ax.text(self.threshold,  ymin+2, &#34;Vth&#34;,  horizontalalignment=&#39;left&#39;)
            ax.vlines(self.threshold, -5e4, 5e4, &#34;r&#34;, linestyles=&#34;dashed&#34;)
        ax.set_ylim(bottom=min(min(y0), ymin)-5, top=self.threshold)
        ax.hlines(0, min(x_points) - abs(min(x_points) / 4), max(x_points) + abs(max(x_points) / 4), &#34;black&#34;,
                  linewidth=.5)
        leg = ax.get_legend()
        labels = [label._text for label in leg.texts] if leg is not None else []
        l = neuron_name
        if current:
            l += &#34; Current OFF&#34;
        labels.append(l)
        if current:
            labels.append(neuron_name+&#34; Current ON&#34;)
        ax.legend(labels)
        plt.show()
        return figure, ax

    def finalize_state_update(self, current_state, return_thresholded_potentials=False, return_dudt=False,
                              return_winners=True, n_winners=1):
        &#34;&#34;&#34;
        Generalized method to save neurons internal state after updates have been calculated, and to calculate
        the return value for the \_\_call\_\_ methods.
        Used to keep the code cleaner.
        Args:
            current_state (Tensor): Tensor of up-to-date states of neurons.
            return_thresholded_potentials (bool): Default: False.
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (int): Default 1.

        Returns:
            Tuple: Return values depends on the selected flags.
            (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;
        # inhibit where refractoriness is not consumed
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        current_state.clamp_(self.resting_potential, None)

        dudt = current_state - self.previous_state
        thresholded = self.get_thresholded_potentials(current_state)
        spiked = thresholded != 0.0

        # emitted spikes are scaled by dt
        spikes = torch.div(thresholded.sign(), self.ts)

        winners = sf.get_k_winners(thresholded, kwta=n_winners, inhibition_radius=0, spikes=spikes)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            non_inihibited_spikes[0, w[0], :, :] = True
        current_state[spiked] = self.v_reset

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        self.previous_state = current_state

        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (current_state,)
        if return_dudt:
            ret += (dudt,)
        if return_winners:
            ret += (winners,)
        return ret</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.AdEx" href="#SpykeTorch.neurons.AdEx">AdEx</a></li>
<li><a title="SpykeTorch.neurons.EIF" href="#SpykeTorch.neurons.EIF">EIF</a></li>
<li><a title="SpykeTorch.neurons.EIF_Simple" href="#SpykeTorch.neurons.EIF_Simple">EIF_Simple</a></li>
<li><a title="SpykeTorch.neurons.HeterogeneousNeuron" href="#SpykeTorch.neurons.HeterogeneousNeuron">HeterogeneousNeuron</a></li>
<li><a title="SpykeTorch.neurons.IF" href="#SpykeTorch.neurons.IF">IF</a></li>
<li><a title="SpykeTorch.neurons.Izhikevich" href="#SpykeTorch.neurons.Izhikevich">Izhikevich</a></li>
<li><a title="SpykeTorch.neurons.LIF" href="#SpykeTorch.neurons.LIF">LIF</a></li>
<li><a title="SpykeTorch.neurons.LIF_Simple" href="#SpykeTorch.neurons.LIF_Simple">LIF_Simple</a></li>
<li><a title="SpykeTorch.neurons.LIF_ode" href="#SpykeTorch.neurons.LIF_ode">LIF_ode</a></li>
<li><a title="SpykeTorch.neurons.QIF" href="#SpykeTorch.neurons.QIF">QIF</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="SpykeTorch.neurons.Neuron.threshold"><code class="name">var <span class="ident">threshold</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def threshold(self):
    return self._threshold</code></pre>
</details>
</dd>
<dt id="SpykeTorch.neurons.Neuron.per_neuron_thresh"><code class="name">var <span class="ident">per_neuron_thresh</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def per_neuron_thresh(self):
    return self._per_neuron_thresh</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="SpykeTorch.neurons.Neuron.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="SpykeTorch.neurons.Neuron.get_params"><code class="name flex">
<span>def <span class="ident">get_params</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_params(self):
    params = {}
    for k, v in self.__dict__.items():
        if not isinstance(v, torch.Tensor):
            params[k] = v
    return params</code></pre>
</details>
</dd>
<dt id="SpykeTorch.neurons.Neuron.get_thresholded_potentials"><code class="name flex">
<span>def <span class="ident">get_thresholded_potentials</span></span>(<span>self, current_state)</span>
</code></dt>
<dd>
<div class="desc"><p>General method to get thresholded membrane potentials, i.e. a Tensor where values are != 0.0 only if they were above
the corresponding neuron's threshold.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>current_state</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>membrane potentials of the neurons</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>thresholded membrane potentials.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_thresholded_potentials(self, current_state):
    &#34;&#34;&#34;
    General method to get thresholded membrane potentials, i.e. a Tensor where values are != 0.0 only if they were above
    the corresponding neuron&#39;s threshold.
    Args:
        current_state (Tensor): membrane potentials of the neurons

    Returns:
        Tensor: thresholded membrane potentials.
    &#34;&#34;&#34;
    thresholded = current_state.clone().detach()

    # inhibit where refractoriness is not consumed
    thresholded[self.refractory_periods &gt; 0] = self.resting_potential

    if self.per_neuron_thresh is None:
        self.per_neuron_thresh = torch.ones(current_state.shape, device=DEVICE)*self.threshold
    if self._threshold is None:
        thresholded[:-1] = 0
    else:
        thresholded[thresholded &lt; self.per_neuron_thresh] = 0.0

    return thresholded</code></pre>
</details>
</dd>
<dt id="SpykeTorch.neurons.Neuron.plot_ode"><code class="name flex">
<span>def <span class="ident">plot_ode</span></span>(<span>self, figure:Â matplotlib.figure.FigureÂ =Â None, ax:Â matplotlib.axes._axes.AxesÂ =Â None, current=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Plots the neuron's ODE. If current is given, ODE with and without current are plotted.
Multiple plots can be stack onto each other by passing the proper figure and axes as an argument.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>figure</code></strong> :&ensp;<code>pyplot.Figure</code></dt>
<dd>Figure to use for plots. If None, a new one is created.</dd>
<dt><strong><code>ax</code></strong> :&ensp;<code>pyplot.Axes</code></dt>
<dd>Axes to use for the plot. If None, a new one is created.</dd>
<dt><strong><code>current</code></strong></dt>
<dd>If provided, draws the current ON/OFF plots.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple</code></dt>
<dd>The figure and axes of the plot.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_ode(self, figure: plt.Figure = None, ax: plt.Axes = None, current=None):
    &#34;&#34;&#34;
    Plots the neuron&#39;s ODE. If current is given, ODE with and without current are plotted.
    Multiple plots can be stack onto each other by passing the proper figure and axes as an argument.
    Args:
        figure (pyplot.Figure): Figure to use for plots. If None, a new one is created.
        ax (pyplot.Axes): Axes to use for the plot. If None, a new one is created.
        current: If provided, draws the current ON/OFF plots.

    Returns:
        Tuple: The figure and axes of the plot.
    &#34;&#34;&#34;
    f = False
    if figure is None:
        figure = plt.figure()
        f = True
    if ax is None:
        ax = figure.add_subplot()  # type: plt.Axes

    neuron_name = self.__str__().split(&#34;_&#34;)[0]
    x_points = np.linspace(self.resting_potential, self.threshold + self.threshold / 2, 300)
    y0 = np.array([self._f_ode(x, 0) for x in x_points])
    ax.plot(x_points, y0)
    if current:
        y1 = np.array([self._f_ode(x, current) for x in x_points])
        ax.plot(x_points, y1)
    xmax = ax.get_xlim()[1]
    ax.set_xlim(min(x_points), max(max(x_points), xmax))
    ymin, ymax = ax.get_ylim()
    if f:
        ax.text(self.threshold,  ymin+2, &#34;Vth&#34;,  horizontalalignment=&#39;left&#39;)
        ax.vlines(self.threshold, -5e4, 5e4, &#34;r&#34;, linestyles=&#34;dashed&#34;)
    ax.set_ylim(bottom=min(min(y0), ymin)-5, top=self.threshold)
    ax.hlines(0, min(x_points) - abs(min(x_points) / 4), max(x_points) + abs(max(x_points) / 4), &#34;black&#34;,
              linewidth=.5)
    leg = ax.get_legend()
    labels = [label._text for label in leg.texts] if leg is not None else []
    l = neuron_name
    if current:
        l += &#34; Current OFF&#34;
    labels.append(l)
    if current:
        labels.append(neuron_name+&#34; Current ON&#34;)
    ax.legend(labels)
    plt.show()
    return figure, ax</code></pre>
</details>
</dd>
<dt id="SpykeTorch.neurons.Neuron.finalize_state_update"><code class="name flex">
<span>def <span class="ident">finalize_state_update</span></span>(<span>self, current_state, return_thresholded_potentials=False, return_dudt=False, return_winners=True, n_winners=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Generalized method to save neurons internal state after updates have been calculated, and to calculate
the return value for the __call__ methods.
Used to keep the code cleaner.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>current_state</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Tensor of up-to-date states of neurons.</dd>
<dt><strong><code>return_thresholded_potentials</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: False.</dd>
<dt><strong><code>return_dudt</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: False.</dd>
<dt><strong><code>return_winners</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: True.</dd>
<dt><strong><code>n_winners</code></strong> :&ensp;<code>int</code></dt>
<dd>Default 1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple</code></dt>
<dd>Return values depends on the selected flags.</dd>
</dl>
<p>(spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def finalize_state_update(self, current_state, return_thresholded_potentials=False, return_dudt=False,
                          return_winners=True, n_winners=1):
    &#34;&#34;&#34;
    Generalized method to save neurons internal state after updates have been calculated, and to calculate
    the return value for the \_\_call\_\_ methods.
    Used to keep the code cleaner.
    Args:
        current_state (Tensor): Tensor of up-to-date states of neurons.
        return_thresholded_potentials (bool): Default: False.
        return_dudt (bool): Default: False.
        return_winners (bool): Default: True.
        n_winners (int): Default 1.

    Returns:
        Tuple: Return values depends on the selected flags.
        (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
    &#34;&#34;&#34;
    # inhibit where refractoriness is not consumed
    current_state[self.refractory_periods &gt; 0] = self.resting_potential
    current_state.clamp_(self.resting_potential, None)

    dudt = current_state - self.previous_state
    thresholded = self.get_thresholded_potentials(current_state)
    spiked = thresholded != 0.0

    # emitted spikes are scaled by dt
    spikes = torch.div(thresholded.sign(), self.ts)

    winners = sf.get_k_winners(thresholded, kwta=n_winners, inhibition_radius=0, spikes=spikes)
    non_inihibited_spikes = torch.full(spiked.shape, False)
    for w in winners:
        non_inihibited_spikes[0, w[0], :, :] = True
    current_state[spiked] = self.v_reset

    # update refractory periods
    self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
    self.refractory_periods[non_inihibited_spikes] = self.refractoriness

    self.previous_state = current_state

    ret = (spikes,)
    if return_thresholded_potentials:
        ret += (thresholded,)
    ret += (current_state,)
    if return_dudt:
        ret += (dudt,)
    if return_winners:
        ret += (winners,)
    return ret</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="SpykeTorch.neurons.IF"><code class="flex name class">
<span>class <span class="ident">IF</span></span>
<span>(</span><span>threshold, tau_rc=0.02, ts=0.001, resting_potential=0.0, refractory_timesteps=2, C=0.281)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates an Integrate and Fire neuron(s) that receives input potentials (from a preceding convolution)
and updates its state according to the amount of PSP received (i.e. if it's enough, it fires a spike).
The neuron(s) state needs to be manually reset when a sequence of related inputs ends (unless the next input is
to be considered as related to the current one as well).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>threshold</code></strong></dt>
<dd>threshold above which the neuron(s) fires a spike</dd>
<dt><strong><code>tau_rc</code></strong></dt>
<dd>the membrane time constant.</dd>
<dt><strong><code>ts</code></strong></dt>
<dd>the time step used for computations, needs to be at least 10 times smaller than tau_rc.</dd>
<dt><strong><code>resting_potential</code></strong></dt>
<dd>potential at which the neuron(s) is set to after a spike.</dd>
<dt><strong><code>refractory_timesteps</code></strong></dt>
<dd>number of timestep of hyperpolarization after a spike.</dd>
<dt><strong><code>C</code></strong></dt>
<dd>Capacitance of the membrane potential. Influences the input potential effect.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IF(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001, resting_potential=0.0, refractory_timesteps=2, C=0.281):

        &#34;&#34;&#34;
        Creates an Integrate and Fire neuron(s) that receives input potentials (from a preceding convolution)
        and updates its state according to the amount of PSP received (i.e. if it&#39;s enough, it fires a spike).
        The neuron(s) state needs to be manually reset when a sequence of related inputs ends (unless the next input is
        to be considered as related to the current one as well).

        Args:
            threshold: threshold above which the neuron(s) fires a spike
            tau_rc: the membrane time constant.
            ts: the time step used for computations, needs to be at least 10 times smaller than tau_rc.
            resting_potential: potential at which the neuron(s) is set to after a spike.
            refractory_timesteps: number of timestep of hyperpolarization after a spike.
            C: Capacitance of the membrane potential. Influences the input potential effect.
        &#34;&#34;&#34;

        # assert tau_rc / ts &gt;= 10  # needs to hold for Taylor series approximation

        super(IF, self).__init__(resting_potential=resting_potential, threshold=threshold)
        self.ts = ts
        self.tau_rc = tau_rc
        self.ts_over_tau = ts / tau_rc  # for better performance (compute once and for all)
        self.exp_term = np.exp(-self.ts_over_tau)  # for better performance (compute once and for all)
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self.refractory_periods = None
        self.C = C

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None

    def __str__(self):
        return &#34;IF_Neuron_rt&#34;+str(self.refractory_timesteps)+&#34;_C&#34;+str(self.C)

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1):
        r&#34;&#34;&#34;Computes the spike-wave tensor from tensor of potentials.
            Args:
                potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
                return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
                return_dudt (bool): Default: False.
                return_winners (bool): Default: True.
                n_winners (bool): Default: 1.
            Returns:
                Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
            &#34;&#34;&#34;

        # potentials = torch.sum(potentials, (2, 3), keepdim=True)
        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
            self.time_since_spike = torch.full(potentials.size(), 0.0, device=DEVICE)

        previous_state = self.previous_state.clone().detach()

        current_state = previous_state.float().clone().detach()


        # Input pulses.
        # In the hypothesis that dt &lt;&lt; tau_rc, we can use Taylor&#39;s expansion to approximate the exponential function.
        # In this way we can more or less simply add the potentials in.
        # input_spikes_impact = potentials * (1 - self.exp_term)

        input_spikes_impact = potentials*self.ts/self.C  # Taylor expansion form (See Neuronal Dynamics Ch.1 Â¶ 1.3.2)
        current_state += input_spikes_impact

        current_state.clip(self.resting_potential, None)

        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0
        # by using this neuron model, spikes are assumed to have amplitude $ A = A_0/t_s $ where A_0 is the spike value
        # (normally 1), and t_s is the time-step size.
        spikes = torch.div(thresholded.sign(), self.ts)
        winners = sf.get_k_winners(thresholded, spikes=spikes, kwta=n_winners)
        # name is non_inhibited_spikes because the corresponding neurons get into refractoriness as if they spiked,
        # even if they haven&#39;t actually spiked.
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            # inhibit all the feature map
            non_inihibited_spikes[0, w[0], :, :] = True

        current_state[spiked] = self.resting_potential
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        dudt = current_state - self.previous_state
        self.previous_state = current_state

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness
        self.time_since_spike += self.ts
        self.time_since_spike[spiked] = 0.0

        # emitted spikes are scaled by dt
        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (current_state,)
        if return_dudt:
            ret += (dudt,)
        if return_winners:
            ret += (winners,)
        return ret</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="SpykeTorch.neurons.IF.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    self.previous_state = None
    self.refractory_periods = None</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.Neuron.finalize_state_update" href="#SpykeTorch.neurons.Neuron.finalize_state_update">finalize_state_update</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.get_thresholded_potentials" href="#SpykeTorch.neurons.Neuron.get_thresholded_potentials">get_thresholded_potentials</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.plot_ode" href="#SpykeTorch.neurons.Neuron.plot_ode">plot_ode</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="SpykeTorch.neurons.LIF"><code class="flex name class">
<span>class <span class="ident">LIF</span></span>
<span>(</span><span>threshold, tau_rc=0.02, ts=0.001, resting_potential=0.0, refractory_timesteps=2, C=0.281, per_neuron_thresh=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Leaky Integrate and Fire neuron(s) that receives input potentials and updates its state according to
the amount of 'energy' received (i.e. if it's enough, it fires a spike).
The neuron(s) state needs to be manually reset when a sequence of related inputs ends (unless the next input is
to be considered as related to the current one as well).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>threshold</code></strong></dt>
<dd>threshold above which the neuron(s) fires a spike</dd>
<dt><strong><code>tau_rc</code></strong></dt>
<dd>the membrane time constant.</dd>
<dt><strong><code>ts</code></strong></dt>
<dd>the time step used for computations, needs to be at least 10 times smaller than tau_rc.</dd>
<dt><strong><code>resting_potential</code></strong></dt>
<dd>potential at which the neuron(s) is set to after a spike.</dd>
<dt><strong><code>refractory_timesteps</code></strong></dt>
<dd>number of timestep of hyperpolarization after a spike.</dd>
<dt><strong><code>C</code></strong></dt>
<dd>Capacitance of the membrane potential. Influences the input potential effect.</dd>
<dt><strong><code>per_neuron_thresh</code></strong></dt>
<dd>defines neuron-wise threshold. If None, a layer-wise threshold is used. Default: None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LIF(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001, resting_potential=0.0, refractory_timesteps=2, C=0.281,
                 per_neuron_thresh=None):

        &#34;&#34;&#34;
        Creates a Leaky Integrate and Fire neuron(s) that receives input potentials and updates its state according to
        the amount of &#39;energy&#39; received (i.e. if it&#39;s enough, it fires a spike).
        The neuron(s) state needs to be manually reset when a sequence of related inputs ends (unless the next input is
        to be considered as related to the current one as well).

        Args:
            threshold: threshold above which the neuron(s) fires a spike
            tau_rc: the membrane time constant.
            ts: the time step used for computations, needs to be at least 10 times smaller than tau_rc.
            resting_potential: potential at which the neuron(s) is set to after a spike.
            refractory_timesteps: number of timestep of hyperpolarization after a spike.
            C: Capacitance of the membrane potential. Influences the input potential effect.
            per_neuron_thresh: defines neuron-wise threshold. If None, a layer-wise threshold is used. Default: None.
        &#34;&#34;&#34;

        # assert tau_rc / ts &gt;= 10  # needs to hold for Taylor series approximation

        Neuron.__init__(self, resting_potential=resting_potential, threshold=threshold)
        self.ts = ts
        self.tau_rc = tau_rc
        self.ts_over_tau = ts / tau_rc  # for better performance (compute once and for all)
        self.exp_term = np.exp(-self.ts_over_tau)  # for better performance (compute once and for all)
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self.refractory_periods = None
        self.C = C
        self.per_neuron_thresh = per_neuron_thresh

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None

    def __str__(self):
        return &#34;LIF_Neuron_rt&#34;+str(self.refractory_timesteps)+&#34;_C&#34;+str(self.C)

    def _f_ode(self, x, I=0):
        return -(x - self.resting_potential) + (self.tau_rc/self.C)*I

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1, return_winning_spikes=False):
        r&#34;&#34;&#34;Computes a (time-) step update for layer of LIF neurons.

            Args:
                potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
                return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
                return_dudt (bool): Default: False.
                return_winners (bool): Default: True.
                n_winners (bool): Default: 1.
            Returns:
                Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
            &#34;&#34;&#34;
        # potentials = torch.sum(potentials, (2, 3), keepdim=True)
        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
            self.time_since_spike = torch.full(potentials.size(), 0.0, device=DEVICE)
            self.time_since_injection = torch.full(potentials.size(), 0.0, device=DEVICE)
            self.potential_at_last_injection = torch.full(potentials.size(), self.resting_potential, device=DEVICE)

        previous_state = self.previous_state.clone().detach()

        # ## adapted from Nengo LIF neuron ## #

        # Exponential decay of the membrane potential.
        # To avoid the need of an extra tensor of time-since-last-spike, we can model it as a difference using
        # a constant exponential time step for the decay and eventually clipping the value to resting_pot.
        # pot = resting_pot + torch.mul((previous_state - resting_pot), np.exp(-dt/tau_rc))
        exp_term = torch.clip(torch.exp(-self.time_since_injection/self.tau_rc), max=1)
        current_state = self.resting_potential + (self.potential_at_last_injection - self.resting_potential) * exp_term

        # Input pulses.
        # In the hypothesis that dt &lt;&lt; tau_rc (at least one order of magnitude), we can use Taylor&#39;s expansion
        # to approximate the exponential function. In this way we can more or less simply add the potentials in.
        # input_spikes_impact = potentials * (1 - self.exp_term)

        input_spikes_impact = potentials*self.ts/self.C  # Taylor expansion form (See Neuronal Dynamics Ch.1 Â¶ 1.3.2)
        current_state += input_spikes_impact
        self.time_since_injection += self.ts
        self.time_since_injection[input_spikes_impact &gt; 0] = self.ts
        # inhibit where refractory period is not yet passed.
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        current_state.clip(self.resting_potential, None)
        self.potential_at_last_injection[input_spikes_impact &gt; 0] = current_state[input_spikes_impact &gt; 0]
        dudt = current_state - self.previous_state
        # resting = torch.full(potentials.size(), resting_pot)
        # delta = torch.add(potentials, -previous_state)
        # delta = torch.add(-previous_state, resting_pot)
        # exp_term = -np.expm1(-dt / leaky_term)
        # delta = torch.mul(delta, exp_term)
        # current_state = torch.add(previous_state, -delta)
        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0
        # by using this neuron model, spikes are assumed to have amplitude $ A = A_0/t_s $ where A_0 is the spike value
        # (here 1), and t_s is the time-step size.
        spikes = torch.div(thresholded.sign(), self.ts)

        winners = sf.get_k_winners(thresholded, spikes=spikes, kwta=n_winners)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            if self.inhibition_mode == &#34;feature&#34;:  # inhibit all the feature map
                non_inihibited_spikes[0, w[0], :, :] = True  # This is then used to inhibit all neurons in the same feature-group of neurons as the one who winned
            elif self.inhibition_mode == &#34;location&#34;:
                non_inihibited_spikes[0, :, w[1], w[2]] = True
            # non_inihibited_spikes[0] = True
        current_state[spiked] = self.resting_potential
        self.previous_state = current_state

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness
        self.time_since_spike += self.ts
        self.time_since_spike[spiked] = 0.0

        # emitted spikes are scaled by dt
        ret = (spikes, )
        if return_thresholded_potentials:
            ret += (thresholded, )
        ret += (current_state, )
        if return_dudt:
            ret += (dudt, )
        if return_winners:
            ret += (winners, )
        if return_winning_spikes:
            not_winning_spikes = torch.full(spiked.shape, True)
            for w in winners:
                not_winning_spikes[0, w[0], w[1], w[2]] = False
            ws = spikes.clone()
            ws[not_winning_spikes] = 0.0
            ret += (ws, )
        return ret</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.UniformLIF" href="#SpykeTorch.neurons.UniformLIF">UniformLIF</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="SpykeTorch.neurons.LIF.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    self.previous_state = None
    self.refractory_periods = None</code></pre>
</details>
</dd>
<dt id="SpykeTorch.neurons.LIF.__call__"><code class="name flex">
<span>def <span class="ident">__call__</span></span>(<span>self, potentials, return_thresholded_potentials=False, return_dudt=False, return_winners=True, n_winners=1, return_winning_spikes=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes a (time-) step update for layer of LIF neurons.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>potentials</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.</dd>
<dt><strong><code>return_thresholded_potentials</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False</dd>
<dt><strong><code>return_dudt</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: False.</dd>
<dt><strong><code>return_winners</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: True.</dd>
<dt><strong><code>n_winners</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: 1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple</code></dt>
<dd>(spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
             return_winners=True, n_winners=1, return_winning_spikes=False):
    r&#34;&#34;&#34;Computes a (time-) step update for layer of LIF neurons.

        Args:
            potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
            return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (bool): Default: 1.
        Returns:
            Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;
    # potentials = torch.sum(potentials, (2, 3), keepdim=True)
    if self.previous_state is None:
        self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
        self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
        self.time_since_spike = torch.full(potentials.size(), 0.0, device=DEVICE)
        self.time_since_injection = torch.full(potentials.size(), 0.0, device=DEVICE)
        self.potential_at_last_injection = torch.full(potentials.size(), self.resting_potential, device=DEVICE)

    previous_state = self.previous_state.clone().detach()

    # ## adapted from Nengo LIF neuron ## #

    # Exponential decay of the membrane potential.
    # To avoid the need of an extra tensor of time-since-last-spike, we can model it as a difference using
    # a constant exponential time step for the decay and eventually clipping the value to resting_pot.
    # pot = resting_pot + torch.mul((previous_state - resting_pot), np.exp(-dt/tau_rc))
    exp_term = torch.clip(torch.exp(-self.time_since_injection/self.tau_rc), max=1)
    current_state = self.resting_potential + (self.potential_at_last_injection - self.resting_potential) * exp_term

    # Input pulses.
    # In the hypothesis that dt &lt;&lt; tau_rc (at least one order of magnitude), we can use Taylor&#39;s expansion
    # to approximate the exponential function. In this way we can more or less simply add the potentials in.
    # input_spikes_impact = potentials * (1 - self.exp_term)

    input_spikes_impact = potentials*self.ts/self.C  # Taylor expansion form (See Neuronal Dynamics Ch.1 Â¶ 1.3.2)
    current_state += input_spikes_impact
    self.time_since_injection += self.ts
    self.time_since_injection[input_spikes_impact &gt; 0] = self.ts
    # inhibit where refractory period is not yet passed.
    current_state[self.refractory_periods &gt; 0] = self.resting_potential
    current_state.clip(self.resting_potential, None)
    self.potential_at_last_injection[input_spikes_impact &gt; 0] = current_state[input_spikes_impact &gt; 0]
    dudt = current_state - self.previous_state
    # resting = torch.full(potentials.size(), resting_pot)
    # delta = torch.add(potentials, -previous_state)
    # delta = torch.add(-previous_state, resting_pot)
    # exp_term = -np.expm1(-dt / leaky_term)
    # delta = torch.mul(delta, exp_term)
    # current_state = torch.add(previous_state, -delta)
    thresholded = self.get_thresholded_potentials(current_state)

    spiked = thresholded != 0.0
    # by using this neuron model, spikes are assumed to have amplitude $ A = A_0/t_s $ where A_0 is the spike value
    # (here 1), and t_s is the time-step size.
    spikes = torch.div(thresholded.sign(), self.ts)

    winners = sf.get_k_winners(thresholded, spikes=spikes, kwta=n_winners)
    non_inihibited_spikes = torch.full(spiked.shape, False)
    for w in winners:
        if self.inhibition_mode == &#34;feature&#34;:  # inhibit all the feature map
            non_inihibited_spikes[0, w[0], :, :] = True  # This is then used to inhibit all neurons in the same feature-group of neurons as the one who winned
        elif self.inhibition_mode == &#34;location&#34;:
            non_inihibited_spikes[0, :, w[1], w[2]] = True
        # non_inihibited_spikes[0] = True
    current_state[spiked] = self.resting_potential
    self.previous_state = current_state

    # update refractory periods
    self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
    self.refractory_periods[non_inihibited_spikes] = self.refractoriness
    self.time_since_spike += self.ts
    self.time_since_spike[spiked] = 0.0

    # emitted spikes are scaled by dt
    ret = (spikes, )
    if return_thresholded_potentials:
        ret += (thresholded, )
    ret += (current_state, )
    if return_dudt:
        ret += (dudt, )
    if return_winners:
        ret += (winners, )
    if return_winning_spikes:
        not_winning_spikes = torch.full(spiked.shape, True)
        for w in winners:
            not_winning_spikes[0, w[0], w[1], w[2]] = False
        ws = spikes.clone()
        ws[not_winning_spikes] = 0.0
        ret += (ws, )
    return ret</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.Neuron.finalize_state_update" href="#SpykeTorch.neurons.Neuron.finalize_state_update">finalize_state_update</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.get_thresholded_potentials" href="#SpykeTorch.neurons.Neuron.get_thresholded_potentials">get_thresholded_potentials</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.plot_ode" href="#SpykeTorch.neurons.Neuron.plot_ode">plot_ode</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="SpykeTorch.neurons.LIF_Simple"><code class="flex name class">
<span>class <span class="ident">LIF_Simple</span></span>
<span>(</span><span>threshold, tau_rc=0.02, ts=0.001, resting_potential=0.0, refractory_timesteps=2, per_neuron_threshold=None)</span>
</code></dt>
<dd>
<div class="desc"><p>A simplified version of the LIF neuron which does not take into account the capacitance and uses a simple decay.
With this class, spikes are propagated with amplitude <span><span class="MathJax_Preview">A = 1</span><script type="math/tex">A = 1</script></span>, instead of <span><span class="MathJax_Preview">A = \frac{1}{t_s}</span><script type="math/tex">A = \frac{1}{t_s}</script></span></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>threshold</code></strong></dt>
<dd>threshold above which the neuron(s) fires a spike</dd>
<dt><strong><code>tau_rc</code></strong></dt>
<dd>the membrane time constant.</dd>
<dt><strong><code>ts</code></strong></dt>
<dd>the time step used for computations, needs to be at least 10 times smaller than tau_rc.</dd>
<dt><strong><code>resting_potential</code></strong></dt>
<dd>potential at which the neuron(s) is set to after a spike.</dd>
<dt><strong><code>refractory_timesteps</code></strong></dt>
<dd>number of timestep of hyperpolarization after a spike.</dd>
<dt><strong><code>C</code></strong></dt>
<dd>Capacitance of the membrane potential. Influences the input potential effect.</dd>
<dt><strong><code>per_neuron_thresh</code></strong></dt>
<dd>Defines neuron-wise threshold. If None, a layer-wise threshold is used. Default: None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LIF_Simple(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001, resting_potential=0.0, refractory_timesteps=2,
                 per_neuron_threshold=None):
        &#34;&#34;&#34;
            A simplified version of the LIF neuron which does not take into account the capacitance and uses a simple decay.
            With this class, spikes are propagated with amplitude \(A = 1\), instead of \(A = \\frac{1}{t_s}\)

            Args:
                threshold: threshold above which the neuron(s) fires a spike
                tau_rc: the membrane time constant.
                ts: the time step used for computations, needs to be at least 10 times smaller than tau_rc.
                resting_potential: potential at which the neuron(s) is set to after a spike.
                refractory_timesteps: number of timestep of hyperpolarization after a spike.
                C: Capacitance of the membrane potential. Influences the input potential effect.
                per_neuron_thresh: Defines neuron-wise threshold. If None, a layer-wise threshold is used. Default: None.
        &#34;&#34;&#34;
        Neuron.__init__(self, resting_potential=resting_potential, threshold=threshold)
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = refractory_timesteps*ts
        self.per_neuron_thresh = per_neuron_threshold
        self.tau_rc = tau_rc
        self.ts = ts
        assert tau_rc &gt; 3*ts  # needed for Taylor approx.; actually would be better with 6 times more than ts
        self.decay = 1 - ts/tau_rc  # Taylor approx

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None

    def __str__(self):
        return &#34;LIF_Simple_RT&#34; + str(self.refractory_timesteps) + &#34;_tau&#34; + str(self.tau_rc)

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1, return_winning_spikes=False):
        &#34;&#34;&#34;
        Calculates a (time-) step update for the layer of LIF neurons.

        Args:
            potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
            return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (bool): Default: 1.
        Returns:
            Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;
        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
        previous_state = self.previous_state.clone().detach()

        current_state = previous_state*self.decay
        current_state += potentials
        current_state[self.refractory_periods &gt; 0] = self.resting_potential

        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0
        spikes = thresholded.sign()

        winners = sf.get_k_winners(thresholded, spikes=spikes, kwta=n_winners)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            if self.inhibition_mode == &#34;feature&#34;:  # inhibit all the feature map
                non_inihibited_spikes[0, w[0], :, :] = True  # This is then used to inhibit all neurons in the same feature-group of neurons as the one who winned
            elif self.inhibition_mode == &#34;location&#34;:
                non_inihibited_spikes[0, :, w[1], w[2]] = True
            # non_inihibited_spikes[0] = True  # to be used in single-neuron scenarios
        current_state[spiked] = self.resting_potential
        self.previous_state = current_state

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        # emitted spikes are NOT scaled by dt
        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (current_state,)
        if return_dudt:
            ret += (current_state-previous_state,)
        if return_winners:
            ret += (winners,)
        if return_winning_spikes:
            not_winning_spikes = torch.full(spiked.shape, True)
            for w in winners:
                not_winning_spikes[0, w[0], w[1], w[2]] = False
            ws = spikes.clone()
            ws[not_winning_spikes] = 0.0
            ret += (ws,)
        return ret</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="SpykeTorch.neurons.LIF_Simple.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    self.previous_state = None
    self.refractory_periods = None</code></pre>
</details>
</dd>
<dt id="SpykeTorch.neurons.LIF_Simple.__call__"><code class="name flex">
<span>def <span class="ident">__call__</span></span>(<span>self, potentials, return_thresholded_potentials=False, return_dudt=False, return_winners=True, n_winners=1, return_winning_spikes=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates a (time-) step update for the layer of LIF neurons.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>potentials</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.</dd>
<dt><strong><code>return_thresholded_potentials</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False</dd>
<dt><strong><code>return_dudt</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: False.</dd>
<dt><strong><code>return_winners</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: True.</dd>
<dt><strong><code>n_winners</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: 1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple</code></dt>
<dd>(spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
             return_winners=True, n_winners=1, return_winning_spikes=False):
    &#34;&#34;&#34;
    Calculates a (time-) step update for the layer of LIF neurons.

    Args:
        potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
        return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
        return_dudt (bool): Default: False.
        return_winners (bool): Default: True.
        n_winners (bool): Default: 1.
    Returns:
        Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
    &#34;&#34;&#34;
    if self.previous_state is None:
        self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
        self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
    previous_state = self.previous_state.clone().detach()

    current_state = previous_state*self.decay
    current_state += potentials
    current_state[self.refractory_periods &gt; 0] = self.resting_potential

    thresholded = self.get_thresholded_potentials(current_state)

    spiked = thresholded != 0.0
    spikes = thresholded.sign()

    winners = sf.get_k_winners(thresholded, spikes=spikes, kwta=n_winners)
    non_inihibited_spikes = torch.full(spiked.shape, False)
    for w in winners:
        if self.inhibition_mode == &#34;feature&#34;:  # inhibit all the feature map
            non_inihibited_spikes[0, w[0], :, :] = True  # This is then used to inhibit all neurons in the same feature-group of neurons as the one who winned
        elif self.inhibition_mode == &#34;location&#34;:
            non_inihibited_spikes[0, :, w[1], w[2]] = True
        # non_inihibited_spikes[0] = True  # to be used in single-neuron scenarios
    current_state[spiked] = self.resting_potential
    self.previous_state = current_state

    # update refractory periods
    self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
    self.refractory_periods[non_inihibited_spikes] = self.refractoriness

    # emitted spikes are NOT scaled by dt
    ret = (spikes,)
    if return_thresholded_potentials:
        ret += (thresholded,)
    ret += (current_state,)
    if return_dudt:
        ret += (current_state-previous_state,)
    if return_winners:
        ret += (winners,)
    if return_winning_spikes:
        not_winning_spikes = torch.full(spiked.shape, True)
        for w in winners:
            not_winning_spikes[0, w[0], w[1], w[2]] = False
        ws = spikes.clone()
        ws[not_winning_spikes] = 0.0
        ret += (ws,)
    return ret</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.Neuron.finalize_state_update" href="#SpykeTorch.neurons.Neuron.finalize_state_update">finalize_state_update</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.get_thresholded_potentials" href="#SpykeTorch.neurons.Neuron.get_thresholded_potentials">get_thresholded_potentials</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.plot_ode" href="#SpykeTorch.neurons.Neuron.plot_ode">plot_ode</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="SpykeTorch.neurons.LIF_ode"><code class="flex name class">
<span>class <span class="ident">LIF_ode</span></span>
<span>(</span><span>threshold, tau_rc=0.02, ts=0.001, resting_potential=0.0, refractory_timesteps=2, C=0.281, per_neuron_thresh=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a Leaky Integrate and Fire neuron(s) that receives input potentials and updates its state according to the amount of 'energy' received (i.e. if it's enough, it fires a spike).
Differently from the LIF class, the LIF_ode uses the LIF ode to directly calculate updates time-step by time-step.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>threshold</code></strong></dt>
<dd>threshold above which the neuron(s) fires a spike</dd>
<dt><strong><code>tau_rc</code></strong></dt>
<dd>the membrane time constant.</dd>
<dt><strong><code>ts</code></strong></dt>
<dd>the time step used for computations, needs to be at least 10 times smaller than tau_rc.</dd>
<dt><strong><code>resting_potential</code></strong></dt>
<dd>potential at which the neuron(s) is set to after a spike.</dd>
<dt><strong><code>refractory_timesteps</code></strong></dt>
<dd>number of timestep of hyperpolarization after a spike.</dd>
<dt><strong><code>C</code></strong></dt>
<dd>Capacitance of the membrane potential. Influences the input potential effect.</dd>
<dt><strong><code>per_neuron_thresh</code></strong></dt>
<dd>Defines neuron-wise threshold. If None, a layer-wise threshold is used. Default: None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LIF_ode(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001, resting_potential=0.0, refractory_timesteps=2, C=0.281,
                 per_neuron_thresh=None):

        &#34;&#34;&#34;
        Creates a Leaky Integrate and Fire neuron(s) that receives input potentials and updates its state according to the amount of &#39;energy&#39; received (i.e. if it&#39;s enough, it fires a spike).
        Differently from the LIF class, the LIF_ode uses the LIF ode to directly calculate updates time-step by time-step.


        Args:
                threshold: threshold above which the neuron(s) fires a spike
                tau_rc: the membrane time constant.
                ts: the time step used for computations, needs to be at least 10 times smaller than tau_rc.
                resting_potential: potential at which the neuron(s) is set to after a spike.
                refractory_timesteps: number of timestep of hyperpolarization after a spike.
                C: Capacitance of the membrane potential. Influences the input potential effect.
                per_neuron_thresh: Defines neuron-wise threshold. If None, a layer-wise threshold is used. Default: None.
        &#34;&#34;&#34;

        # assert tau_rc / ts &gt;= 10  # needs to hold for Taylor series approximation

        super(LIF_ode, self).__init__(resting_potential=resting_potential, threshold=threshold)
        self.ts = ts
        self.tau_rc = tau_rc
        self.ts_over_tau = ts / tau_rc  # for better performance (compute once and for all)
        self.exp_term = np.exp(-self.ts_over_tau)  # for better performance (compute once and for all)
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self.refractory_periods = None
        self.C = C
        self.per_neuron_thresh = per_neuron_thresh

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None

    def __str__(self):
        return &#34;LIF_ode_Neuron_rt&#34;+str(self.refractory_timesteps)+&#34;_C&#34;+str(self.C)

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1):
        &#34;&#34;&#34;Computes a (time-) step update for the layer of LIF neurons.
            Args:
                potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
                return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
                return_dudt (bool): Default: False.
                return_winners (bool): Default: True.
                n_winners (bool): Default: 1.
            Returns:
                Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
            &#34;&#34;&#34;
        # potentials = torch.sum(potentials, (2, 3), keepdim=True)
        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)

        previous_state = self.previous_state.clone().detach()

        du = (self.resting_potential - previous_state)*self.ts_over_tau
        du += potentials * self.ts / self.C

        current_state = previous_state + du

        # inhibit where refractory period is not yet passed.
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        current_state.clip(self.resting_potential, None)

        dudt = current_state - self.previous_state

        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0
        spikes = torch.div(thresholded.sign(), self.ts)

        winners = sf.get_k_winners(thresholded, spikes=spikes, kwta=n_winners)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            # inhibit the whole feature map for the next iteration. One feature map = one feature.
            # One neuron firing in one feature map = one feature at that position.
            non_inihibited_spikes[0, w[0], :, :] = True
            # non_inihibited_spikes[0] = True
        current_state[spiked] = self.resting_potential

        self.previous_state = current_state

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        # emitted spikes are scaled by dt
        ret = (spikes, )
        if return_thresholded_potentials:
            ret += (thresholded, )
        ret += (current_state, )
        if return_dudt:
            ret += (dudt, )
        if return_winners:
            ret += (winners, )
        return ret</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="SpykeTorch.neurons.LIF_ode.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    self.previous_state = None
    self.refractory_periods = None</code></pre>
</details>
</dd>
<dt id="SpykeTorch.neurons.LIF_ode.__call__"><code class="name flex">
<span>def <span class="ident">__call__</span></span>(<span>self, potentials, return_thresholded_potentials=False, return_dudt=False, return_winners=True, n_winners=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes a (time-) step update for the layer of LIF neurons.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>potentials</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.</dd>
<dt><strong><code>return_thresholded_potentials</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False</dd>
<dt><strong><code>return_dudt</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: False.</dd>
<dt><strong><code>return_winners</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: True.</dd>
<dt><strong><code>n_winners</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: 1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple</code></dt>
<dd>(spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
             return_winners=True, n_winners=1):
    &#34;&#34;&#34;Computes a (time-) step update for the layer of LIF neurons.
        Args:
            potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
            return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (bool): Default: 1.
        Returns:
            Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;
    # potentials = torch.sum(potentials, (2, 3), keepdim=True)
    if self.previous_state is None:
        self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
        self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)

    previous_state = self.previous_state.clone().detach()

    du = (self.resting_potential - previous_state)*self.ts_over_tau
    du += potentials * self.ts / self.C

    current_state = previous_state + du

    # inhibit where refractory period is not yet passed.
    current_state[self.refractory_periods &gt; 0] = self.resting_potential
    current_state.clip(self.resting_potential, None)

    dudt = current_state - self.previous_state

    thresholded = self.get_thresholded_potentials(current_state)

    spiked = thresholded != 0.0
    spikes = torch.div(thresholded.sign(), self.ts)

    winners = sf.get_k_winners(thresholded, spikes=spikes, kwta=n_winners)
    non_inihibited_spikes = torch.full(spiked.shape, False)
    for w in winners:
        # inhibit the whole feature map for the next iteration. One feature map = one feature.
        # One neuron firing in one feature map = one feature at that position.
        non_inihibited_spikes[0, w[0], :, :] = True
        # non_inihibited_spikes[0] = True
    current_state[spiked] = self.resting_potential

    self.previous_state = current_state

    # update refractory periods
    self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
    self.refractory_periods[non_inihibited_spikes] = self.refractoriness

    # emitted spikes are scaled by dt
    ret = (spikes, )
    if return_thresholded_potentials:
        ret += (thresholded, )
    ret += (current_state, )
    if return_dudt:
        ret += (dudt, )
    if return_winners:
        ret += (winners, )
    return ret</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.Neuron.finalize_state_update" href="#SpykeTorch.neurons.Neuron.finalize_state_update">finalize_state_update</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.get_thresholded_potentials" href="#SpykeTorch.neurons.Neuron.get_thresholded_potentials">get_thresholded_potentials</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.plot_ode" href="#SpykeTorch.neurons.Neuron.plot_ode">plot_ode</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="SpykeTorch.neurons.EIF"><code class="flex name class">
<span>class <span class="ident">EIF</span></span>
<span>(</span><span>threshold, tau_rc=0.02, ts=0.001, delta_t=0.5, theta_rh=None, resting_potential=0.0, refractory_timesteps=2, C=0.281, v_reset=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a layer of exponential integrate and fire neurons.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>threshold</code></strong></dt>
<dd>Default: None.</dd>
<dt><strong><code>tau_rc</code></strong></dt>
<dd>Membrane time constant a.k.a. tau_m or tau in seconds. Default: 0.02.</dd>
<dt><strong><code>ts</code></strong></dt>
<dd>time-step value in seconds. Default: 0.001.</dd>
<dt><strong><code>delta_t</code></strong></dt>
<dd>Sharpness parameter (upswing on the exponential curve). If ~0, EIF behaves like LIF. Default: 0.5.</dd>
<dt><strong><code>theta_rh</code></strong></dt>
<dd>Rheobase threshold. Default: None.</dd>
<dt><strong><code>resting_potential</code></strong></dt>
<dd>Default: 0.0.</dd>
<dt><strong><code>refractory_timesteps</code></strong></dt>
<dd>Default: 2.</dd>
<dt><strong><code>C</code></strong></dt>
<dd>Capacitance. Default: 0.281.</dd>
<dt><strong><code>v_reset</code></strong></dt>
<dd>Default: None.</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;<code>theta_rh</code> being <code>None</code> will cause <code>theta_rh</code> to be <span><span class="MathJax_Preview">\frac{3}{4}V_{thresh}</span><script type="math/tex">\frac{3}{4}V_{thresh}</script></span>.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EIF(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001, delta_t=0.5, theta_rh=None, resting_potential=0.0,
                 refractory_timesteps=2, C=0.281, v_reset=None):
        &#34;&#34;&#34;
        Creates a layer of exponential integrate and fire neurons.
        Args:
            threshold: Default: None.
            tau_rc: Membrane time constant a.k.a. tau_m or tau in seconds. Default: 0.02.
            ts: time-step value in seconds. Default: 0.001.
            delta_t: Sharpness parameter (upswing on the exponential curve). If ~0, EIF behaves like LIF. Default: 0.5.
            theta_rh: Rheobase threshold. Default: None.
            resting_potential: Default: 0.0.
            refractory_timesteps: Default: 2.
            C: Capacitance. Default: 0.281.
            v_reset: Default: None.


        .. note:: `theta_rh` being `None` will cause `theta_rh` to be \(\\frac{3}{4}V_{thresh}\).
        &#34;&#34;&#34;
        Neuron.__init__(self, resting_potential=resting_potential, threshold=threshold)

        # assert abs(theta_rh / (resting_potential + delta_t)) &gt;= 10, \
        #     &#34;Needs to hold as it is assumed in Neuronal Dynamics book, Ch.5 Â¶ 5.2&#34;
        self.tau_rc = tau_rc
        self.ts = ts
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self.refractory_periods = None
        self.C = C
        self.delta_t = delta_t
        if theta_rh is None:  # guess from threshold
            theta_rh = -abs(threshold)*0.25 + threshold  # make the rheobase threshold smaller of the threshold by 25%
        self.theta_rh = theta_rh
        if v_reset is None:
            self.v_reset = resting_potential
        else:
            self.v_reset = v_reset

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None

    def __str__(self):
        return &#34;EIF_Neuron_rt&#34;+str(self.refractory_timesteps)+&#34;_C&#34;+str(self.C)

    def _f_ode(self, x, I=0):
        return -(x - self.resting_potential) + self.delta_t*np.exp((x-self.theta_rh)/self.delta_t) + (self.tau_rc / self.C) * I

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1):
        &#34;&#34;&#34;
        Calculates the (time-) step update for the neurons as specified by the following differential equation:
        $$
            \\tau_{rc}\\frac{du}{dt} = -(u - u_{rest}) + \\Delta_T \\cdot \\exp\\left({\\frac{u - \\Theta_{rh}}{\\Delta_T}}\\right)
            + R\\cdot I(t)
        $$
        Args:
            potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
            return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (bool): Default: 1.
        Returns:
            Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;
        # potentials = torch.sum(potentials, (2, 3), keepdim=True)

        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)

        previous_state = self.previous_state.clone().detach()

        du = self.resting_potential - previous_state  # -(previous_state - self.resting_potential)
        du = du + self.delta_t * torch.exp((previous_state - self.theta_rh) / self.delta_t)
        du /= self.tau_rc
        du += potentials/self.C
        du *= self.ts
        current_state = previous_state + du

        # inhibit where refractoriness is not consumed
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        current_state.clamp_(self.resting_potential, None)

        dudt = current_state - self.previous_state
        # current_state.clip(self.resting_potential, None)

        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0

        # emitted spikes are scaled by dt
        spikes = torch.div(thresholded.sign(), self.ts)

        winners = sf.get_k_winners(thresholded, kwta=n_winners, inhibition_radius=0, spikes=spikes)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            non_inihibited_spikes[0, w[0], :, :] = True
        current_state[spiked] = self.v_reset

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        self.previous_state = current_state

        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (current_state,)
        if return_dudt:
            ret += (dudt,)
        if return_winners:
            ret += (winners,)
        return ret</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.UniformEIF" href="#SpykeTorch.neurons.UniformEIF">UniformEIF</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="SpykeTorch.neurons.EIF.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    self.previous_state = None
    self.refractory_periods = None</code></pre>
</details>
</dd>
<dt id="SpykeTorch.neurons.EIF.__call__"><code class="name flex">
<span>def <span class="ident">__call__</span></span>(<span>self, potentials, return_thresholded_potentials=False, return_dudt=False, return_winners=True, n_winners=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the (time-) step update for the neurons as specified by the following differential equation:
<span><span class="MathJax_Preview">
\tau_{rc}\frac{du}{dt} = -(u - u_{rest}) + \Delta_T \cdot \exp\left({\frac{u - \Theta_{rh}}{\Delta_T}}\right)
+ R\cdot I(t)
</span><script type="math/tex; mode=display">
\tau_{rc}\frac{du}{dt} = -(u - u_{rest}) + \Delta_T \cdot \exp\left({\frac{u - \Theta_{rh}}{\Delta_T}}\right)
+ R\cdot I(t)
</script></span></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>potentials</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.</dd>
<dt><strong><code>return_thresholded_potentials</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False</dd>
<dt><strong><code>return_dudt</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: False.</dd>
<dt><strong><code>return_winners</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: True.</dd>
<dt><strong><code>n_winners</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: 1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple</code></dt>
<dd>(spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
             return_winners=True, n_winners=1):
    &#34;&#34;&#34;
    Calculates the (time-) step update for the neurons as specified by the following differential equation:
    $$
        \\tau_{rc}\\frac{du}{dt} = -(u - u_{rest}) + \\Delta_T \\cdot \\exp\\left({\\frac{u - \\Theta_{rh}}{\\Delta_T}}\\right)
        + R\\cdot I(t)
    $$
    Args:
        potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
        return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
        return_dudt (bool): Default: False.
        return_winners (bool): Default: True.
        n_winners (bool): Default: 1.
    Returns:
        Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
    &#34;&#34;&#34;
    # potentials = torch.sum(potentials, (2, 3), keepdim=True)

    if self.previous_state is None:
        self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
        self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)

    previous_state = self.previous_state.clone().detach()

    du = self.resting_potential - previous_state  # -(previous_state - self.resting_potential)
    du = du + self.delta_t * torch.exp((previous_state - self.theta_rh) / self.delta_t)
    du /= self.tau_rc
    du += potentials/self.C
    du *= self.ts
    current_state = previous_state + du

    # inhibit where refractoriness is not consumed
    current_state[self.refractory_periods &gt; 0] = self.resting_potential
    current_state.clamp_(self.resting_potential, None)

    dudt = current_state - self.previous_state
    # current_state.clip(self.resting_potential, None)

    thresholded = self.get_thresholded_potentials(current_state)

    spiked = thresholded != 0.0

    # emitted spikes are scaled by dt
    spikes = torch.div(thresholded.sign(), self.ts)

    winners = sf.get_k_winners(thresholded, kwta=n_winners, inhibition_radius=0, spikes=spikes)
    non_inihibited_spikes = torch.full(spiked.shape, False)
    for w in winners:
        non_inihibited_spikes[0, w[0], :, :] = True
    current_state[spiked] = self.v_reset

    # update refractory periods
    self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
    self.refractory_periods[non_inihibited_spikes] = self.refractoriness

    self.previous_state = current_state

    ret = (spikes,)
    if return_thresholded_potentials:
        ret += (thresholded,)
    ret += (current_state,)
    if return_dudt:
        ret += (dudt,)
    if return_winners:
        ret += (winners,)
    return ret</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.Neuron.finalize_state_update" href="#SpykeTorch.neurons.Neuron.finalize_state_update">finalize_state_update</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.get_thresholded_potentials" href="#SpykeTorch.neurons.Neuron.get_thresholded_potentials">get_thresholded_potentials</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.plot_ode" href="#SpykeTorch.neurons.Neuron.plot_ode">plot_ode</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="SpykeTorch.neurons.EIF_Simple"><code class="flex name class">
<span>class <span class="ident">EIF_Simple</span></span>
<span>(</span><span>threshold, tau_rc=0.02, ts=0.001, delta_t=0.5, theta_rh=None, resting_potential=0.0, refractory_timesteps=2, v_reset=None, per_neuron_threshold=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a layer of exponential integrate and fire neurons. These neurons are simplified with respect to the EIF class, in the sense that the capacitance is not used anymore, the linear decay is implemented through a simple multiplication and the incoming potentials are not expected to be scaled by <span><span class="MathJax_Preview">\frac{1}{t_s}</span><script type="math/tex">\frac{1}{t_s}</script></span>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>threshold</code></strong></dt>
<dd>Default: None.</dd>
<dt><strong><code>tau_rc</code></strong></dt>
<dd>Membrane time constant a.k.a. tau_m or tau in seconds. Default: 0.02.</dd>
<dt><strong><code>ts</code></strong></dt>
<dd>time-step value in seconds. Default: 0.001.</dd>
<dt><strong><code>delta_t</code></strong></dt>
<dd>Sharpness parameter (upswing on the exponential curve). If ~0, EIF behaves like LIF. Default: 0.5.</dd>
<dt><strong><code>theta_rh</code></strong></dt>
<dd>Rheobase threshold. Default: None.</dd>
<dt><strong><code>resting_potential</code></strong></dt>
<dd>Default: 0.0.</dd>
<dt><strong><code>refractory_timesteps</code></strong></dt>
<dd>Default: 2.</dd>
<dt><strong><code>C</code></strong></dt>
<dd>Capacitance. Default: 0.281.</dd>
<dt><strong><code>v_reset</code></strong></dt>
<dd>Default: None.</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;<code>theta_rh</code> being <code>None</code> will cause <code>theta_rh</code> to be <span><span class="MathJax_Preview">\frac{3}{4}V_{thresh}</span><script type="math/tex">\frac{3}{4}V_{thresh}</script></span>.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EIF_Simple(Neuron):
    def __init__(self, threshold, tau_rc=0.02, ts=0.001, delta_t=0.5, theta_rh=None, resting_potential=0.0,
                 refractory_timesteps=2, v_reset=None, per_neuron_threshold=None):
        &#34;&#34;&#34;
        Creates a layer of exponential integrate and fire neurons. These neurons are simplified with respect to the EIF class, in the sense that the capacitance is not used anymore, the linear decay is implemented through a simple multiplication and the incoming potentials are not expected to be scaled by \(\\frac{1}{t_s}\).
        Args:
            threshold: Default: None.
            tau_rc: Membrane time constant a.k.a. tau_m or tau in seconds. Default: 0.02.
            ts: time-step value in seconds. Default: 0.001.
            delta_t: Sharpness parameter (upswing on the exponential curve). If ~0, EIF behaves like LIF. Default: 0.5.
            theta_rh: Rheobase threshold. Default: None.
            resting_potential: Default: 0.0.
            refractory_timesteps: Default: 2.
            C: Capacitance. Default: 0.281.
            v_reset: Default: None.

        .. note:: `theta_rh` being `None` will cause `theta_rh` to be \(\\frac{3}{4}V_{thresh}\).
        &#34;&#34;&#34;
        Neuron.__init__(self, resting_potential=resting_potential, threshold=threshold)
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self._per_neuron_thresh = per_neuron_threshold
        self.tau_rc = tau_rc
        self.ts = ts
        assert tau_rc &gt; 3*ts  # needed for Taylor approx.; actually would be better with 6 times more than ts
        self.decay = 1 - ts/tau_rc  # Taylor approx

        self.delta_t = delta_t
        if theta_rh is None:  # guess from threshold
            theta_rh = -abs(threshold)*0.25 + threshold  # make the rheobase threshold smaller of the threshold by 25%
        self.theta_rh = theta_rh
        if v_reset is None:
            self.v_reset = resting_potential
        else:
            self.v_reset = v_reset

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None

    @Neuron.per_neuron_thresh.setter
    def per_neuron_thresh(self, value):
        self._per_neuron_thresh = value
        self.theta_rh = 0.75*value

    def __str__(self):
        return &#34;EIF_Simple_RT&#34; + str(self.refractory_timesteps) + &#34;_tau&#34; + str(self.tau_rc)

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1, return_winning_spikes=False):
        &#34;&#34;&#34;
        Calculates the (time-) step update for the neurons as specified by the following differential equation:
        $$
            \\tau_{rc}\\frac{du}{dt} = -(u - u_{rest}) + \\Delta_T \\cdot \\exp\\left({\\frac{u - \\Theta_{rh}}{\\Delta_T}}\\right)
            + R\\cdot I(t)
        $$
        Args:
            potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
            return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (bool): Default: 1.
        Returns:
            Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;
        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
        previous_state = self.previous_state.clone().detach()

        current_state = previous_state*self.decay + self.delta_t * torch.exp((previous_state - self.theta_rh) / self.delta_t)
        current_state += potentials
        current_state[self.refractory_periods &gt; 0] = self.resting_potential

        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0
        spikes = thresholded.sign()

        winners = sf.get_k_winners(thresholded, spikes=spikes, kwta=n_winners)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            if self.inhibition_mode == &#34;feature&#34;:  # inhibit all the feature map
                non_inihibited_spikes[0, w[0], :, :] = True  # This is then used to inhibit all neurons in the same feature-group of neurons as the one who winned
            elif self.inhibition_mode == &#34;location&#34;:
                non_inihibited_spikes[0, :, w[1], w[2]] = True
            # non_inihibited_spikes[0] = True
        # neurons that fired a spike a reset to v_reset regardless of being winners
        membrane_potential = current_state.clone().detach()
        current_state[spiked] = self.v_reset
        self.previous_state = current_state

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        # but only the winners (and the inhibited neurons) are given a refractory period.
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        # emitted spikes are NOT scaled by dt
        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (membrane_potential,)  # (current_state,)
        if return_dudt:
            ret += (current_state-previous_state,)  # this will result in very negative dudt sometimes, can be done better
        if return_winners:
            ret += (winners,)
        if return_winning_spikes:
            not_winning_spikes = torch.full(spiked.shape, True)
            for w in winners:
                not_winning_spikes[0, w[0], w[1], w[2]] = False
            ws = spikes.clone()
            ws[not_winning_spikes] = 0.0
            ret += (ws,)
        return ret</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="SpykeTorch.neurons.EIF_Simple.per_neuron_thresh"><code class="name">var <span class="ident">per_neuron_thresh</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def per_neuron_thresh(self):
    return self._per_neuron_thresh</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="SpykeTorch.neurons.EIF_Simple.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    self.previous_state = None
    self.refractory_periods = None</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.Neuron.finalize_state_update" href="#SpykeTorch.neurons.Neuron.finalize_state_update">finalize_state_update</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.get_thresholded_potentials" href="#SpykeTorch.neurons.Neuron.get_thresholded_potentials">get_thresholded_potentials</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.plot_ode" href="#SpykeTorch.neurons.Neuron.plot_ode">plot_ode</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="SpykeTorch.neurons.AdEx"><code class="flex name class">
<span>class <span class="ident">AdEx</span></span>
<span>(</span><span>threshold, tau_rc=0.02, ts=0.001, delta_t=0.5, theta_rh=None, resting_potential=0.0, refractory_timesteps=2, C=0.281, v_reset=None, a=0.6, b=0.7, tau_w=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a layer of Adaptive Exponential Integrate and Fire (AdEx) neurons.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>threshold</code></strong></dt>
<dd>Default: None.</dd>
<dt><strong><code>tau_rc</code></strong></dt>
<dd>Membrane time constant a.k.a. tau_m or tau, in seconds. Default: 0.02.</dd>
<dt><strong><code>ts</code></strong></dt>
<dd>time-step value in seconds. Default: 0.001.</dd>
<dt><strong><code>delta_t</code></strong></dt>
<dd>Sharpness parameter (upswing on the exponential curve). If ~0, EIF behaves like LIF. Default: 0.5.</dd>
<dt><strong><code>theta_rh</code></strong></dt>
<dd>Rheobase threshold, if None it's equal to <span><span class="MathJax_Preview">\frac{3}{4}V_{thresh}</span><script type="math/tex">\frac{3}{4}V_{thresh}</script></span>. Default: None.</dd>
<dt><strong><code>resting_potential</code></strong></dt>
<dd>Default: 0.0.</dd>
<dt><strong><code>refractory_timesteps</code></strong></dt>
<dd>Default: 2.</dd>
<dt><strong><code>C</code></strong></dt>
<dd>Capacitance. Default: 0.281.</dd>
<dt><strong><code>v_reset</code></strong></dt>
<dd>After-spike reset voltage, if None it is equal to the resting potential. Default: None.</dd>
<dt><strong><code>a</code></strong></dt>
<dd>Adaptation variable parameter to regulate the adaptation dependence from the membrane potential. Default: 0.6.</dd>
<dt><strong><code>b</code></strong></dt>
<dd>Adaptation variable parameter to regulate the adaptation increase upon emission of a spike. Default: 0.7.</dd>
<dt><strong><code>tau_w</code></strong></dt>
<dd>Adaptation variable time constant. Default: 1.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AdEx(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001, delta_t=0.5,
                 theta_rh=None, resting_potential=0.0,
                 refractory_timesteps=2, C=0.281, v_reset=None,
                 a=0.6, b=0.7, tau_w=1):
        &#34;&#34;&#34;
        Creates a layer of Adaptive Exponential Integrate and Fire (AdEx) neurons.
        Args:
            threshold: Default: None.
            tau_rc: Membrane time constant a.k.a. tau_m or tau, in seconds. Default: 0.02.
            ts: time-step value in seconds. Default: 0.001.
            delta_t: Sharpness parameter (upswing on the exponential curve). If ~0, EIF behaves like LIF. Default: 0.5.
            theta_rh: Rheobase threshold, if None it&#39;s equal to \(\\frac{3}{4}V_{thresh}\). Default: None.
            resting_potential: Default: 0.0.
            refractory_timesteps: Default: 2.
            C: Capacitance. Default: 0.281.
            v_reset: After-spike reset voltage, if None it is equal to the resting potential. Default: None.
            a: Adaptation variable parameter to regulate the adaptation dependence from the membrane potential. Default: 0.6.
            b: Adaptation variable parameter to regulate the adaptation increase upon emission of a spike. Default: 0.7.
            tau_w: Adaptation variable time constant. Default: 1.
        &#34;&#34;&#34;
        super(AdEx, self).__init__(resting_potential=resting_potential, threshold=threshold)

        # assert abs(theta_rh / (resting_potential + delta_t)) &gt;= 10, \
        #     &#34;Needs to hold as it is assumed in Neuronal Dynamics book, Ch.5 Â¶ 5.2&#34;
        self.tau_rc = tau_rc
        self.ts = ts
        self.delta_t = delta_t
        if theta_rh is None:  # guess from threshold
            theta_rh = -abs(threshold)*0.25 + threshold  # make the rheobase threshold smaller of the threshold by 25%
        self.theta_rh = theta_rh
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self.refractory_periods = None
        self.C = C
        self.R = tau_rc/C
        self.a = a
        self.b = b
        self.tau_w = tau_w
        if v_reset is None:
            self.v_reset = resting_potential
        else:
            self.v_reset = v_reset

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None
        self.w = None

    def __str__(self):
        return &#34;AdEx_Neuron_rt&#34;+str(self.refractory_timesteps)+&#34;_C&#34;+str(self.C)

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1):
        &#34;&#34;&#34;
        Calculates a (time-) step update for the neuron(s) as specified by the following differential equations:
        $$
            \\tau_{rc}\\frac{du}{dt} = -(u - u_{rest}) + \\Delta_T \\cdot \\exp\\left({\\frac{u - \\Theta_{rh}}{\\Delta_T}}\\right)
            - R\\cdot \\omega + R\\cdot I(t) \\\\
            \\tau_w\\frac{d\\omega}{dt} = a(u - u_{rest}) + b\\sum_{t^{(f)}}\\delta(t-t^{(f)})
        $$
        Args:
            potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
            return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (bool): Default: 1.
        Returns:
            Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;
        # potentials = torch.sum(potentials, (2, 3), keepdim=True)

        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
            self.w = torch.full(potentials.size(), 0.0, device=DEVICE)

        previous_state = self.previous_state.clone().detach()

        du = self.resting_potential - previous_state  # -(previous_state - self.resting_potential)
        du = du + self.delta_t * torch.exp((previous_state - self.theta_rh) / self.delta_t)
        currents_impact = (potentials - self.w)*self.R
        du += currents_impact
        du /= self.tau_rc
        du *= self.ts
        current_state = previous_state + du

        # inhibit where refractoriness is not consumed
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        current_state.clamp_(self.resting_potential, None)

        dudt = current_state - self.previous_state
        # current_state.clip(self.resting_potential, None)

        # TODO: Maybe, given that a proper assumption according to Neuronal Dynamics book, would be to have
        # TODO: threshold &gt;&gt; theta_rh + delta_t, if it is None I could set it to be 1 order of magnitude greater?
        # TODO: i.e. threshold = (delta_T + theta_rh) * 10
        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0

        # Implement the common adaptation variable update for all the neurons
        self.w += (self.a*(current_state-self.resting_potential) - self.w)/self.tau_w*self.ts
        # Add a current jump only where there has been a spike
        self.w[spiked] += self.b  # see https://journals.physiology.org/doi/pdf/10.1152/jn.00686.2005 for
        #                         # why this is simply added in.

        # emitted spikes are scaled by dt
        spikes = torch.div(torch.abs(thresholded.sign()), self.ts)

        winners = sf.get_k_winners(thresholded, kwta=n_winners, inhibition_radius=0, spikes=spikes)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            # non_inihibited_spikes[0, w[0], :, :] = True
            non_inihibited_spikes[0] = True
        current_state[spiked] = self.v_reset

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        self.previous_state = current_state

        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (current_state,)
        if return_dudt:
            ret += (dudt,)
        if return_winners:
            ret += (winners,)
        return ret</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="SpykeTorch.neurons.AdEx.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    self.previous_state = None
    self.refractory_periods = None
    self.w = None</code></pre>
</details>
</dd>
<dt id="SpykeTorch.neurons.AdEx.__call__"><code class="name flex">
<span>def <span class="ident">__call__</span></span>(<span>self, potentials, return_thresholded_potentials=False, return_dudt=False, return_winners=True, n_winners=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates a (time-) step update for the neuron(s) as specified by the following differential equations:
<span><span class="MathJax_Preview">
\tau_{rc}\frac{du}{dt} = -(u - u_{rest}) + \Delta_T \cdot \exp\left({\frac{u - \Theta_{rh}}{\Delta_T}}\right)
- R\cdot \omega + R\cdot I(t) \\
\tau_w\frac{d\omega}{dt} = a(u - u_{rest}) + b\sum_{t^{(f)}}\delta(t-t^{(f)})
</span><script type="math/tex; mode=display">
\tau_{rc}\frac{du}{dt} = -(u - u_{rest}) + \Delta_T \cdot \exp\left({\frac{u - \Theta_{rh}}{\Delta_T}}\right)
- R\cdot \omega + R\cdot I(t) \\
\tau_w\frac{d\omega}{dt} = a(u - u_{rest}) + b\sum_{t^{(f)}}\delta(t-t^{(f)})
</script></span></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>potentials</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.</dd>
<dt><strong><code>return_thresholded_potentials</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False</dd>
<dt><strong><code>return_dudt</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: False.</dd>
<dt><strong><code>return_winners</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: True.</dd>
<dt><strong><code>n_winners</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: 1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple</code></dt>
<dd>(spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
             return_winners=True, n_winners=1):
    &#34;&#34;&#34;
    Calculates a (time-) step update for the neuron(s) as specified by the following differential equations:
    $$
        \\tau_{rc}\\frac{du}{dt} = -(u - u_{rest}) + \\Delta_T \\cdot \\exp\\left({\\frac{u - \\Theta_{rh}}{\\Delta_T}}\\right)
        - R\\cdot \\omega + R\\cdot I(t) \\\\
        \\tau_w\\frac{d\\omega}{dt} = a(u - u_{rest}) + b\\sum_{t^{(f)}}\\delta(t-t^{(f)})
    $$
    Args:
        potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
        return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
        return_dudt (bool): Default: False.
        return_winners (bool): Default: True.
        n_winners (bool): Default: 1.
    Returns:
        Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
    &#34;&#34;&#34;
    # potentials = torch.sum(potentials, (2, 3), keepdim=True)

    if self.previous_state is None:
        self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
        self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
        self.w = torch.full(potentials.size(), 0.0, device=DEVICE)

    previous_state = self.previous_state.clone().detach()

    du = self.resting_potential - previous_state  # -(previous_state - self.resting_potential)
    du = du + self.delta_t * torch.exp((previous_state - self.theta_rh) / self.delta_t)
    currents_impact = (potentials - self.w)*self.R
    du += currents_impact
    du /= self.tau_rc
    du *= self.ts
    current_state = previous_state + du

    # inhibit where refractoriness is not consumed
    current_state[self.refractory_periods &gt; 0] = self.resting_potential
    current_state.clamp_(self.resting_potential, None)

    dudt = current_state - self.previous_state
    # current_state.clip(self.resting_potential, None)

    # TODO: Maybe, given that a proper assumption according to Neuronal Dynamics book, would be to have
    # TODO: threshold &gt;&gt; theta_rh + delta_t, if it is None I could set it to be 1 order of magnitude greater?
    # TODO: i.e. threshold = (delta_T + theta_rh) * 10
    thresholded = self.get_thresholded_potentials(current_state)

    spiked = thresholded != 0.0

    # Implement the common adaptation variable update for all the neurons
    self.w += (self.a*(current_state-self.resting_potential) - self.w)/self.tau_w*self.ts
    # Add a current jump only where there has been a spike
    self.w[spiked] += self.b  # see https://journals.physiology.org/doi/pdf/10.1152/jn.00686.2005 for
    #                         # why this is simply added in.

    # emitted spikes are scaled by dt
    spikes = torch.div(torch.abs(thresholded.sign()), self.ts)

    winners = sf.get_k_winners(thresholded, kwta=n_winners, inhibition_radius=0, spikes=spikes)
    non_inihibited_spikes = torch.full(spiked.shape, False)
    for w in winners:
        # non_inihibited_spikes[0, w[0], :, :] = True
        non_inihibited_spikes[0] = True
    current_state[spiked] = self.v_reset

    # update refractory periods
    self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
    self.refractory_periods[non_inihibited_spikes] = self.refractoriness

    self.previous_state = current_state

    ret = (spikes,)
    if return_thresholded_potentials:
        ret += (thresholded,)
    ret += (current_state,)
    if return_dudt:
        ret += (dudt,)
    if return_winners:
        ret += (winners,)
    return ret</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.Neuron.finalize_state_update" href="#SpykeTorch.neurons.Neuron.finalize_state_update">finalize_state_update</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.get_thresholded_potentials" href="#SpykeTorch.neurons.Neuron.get_thresholded_potentials">get_thresholded_potentials</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.plot_ode" href="#SpykeTorch.neurons.Neuron.plot_ode">plot_ode</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="SpykeTorch.neurons.QIF"><code class="flex name class">
<span>class <span class="ident">QIF</span></span>
<span>(</span><span>threshold, tau_rc=0.02, ts=0.001, u_c=None, a=0.001, resting_potential=0.0, refractory_timesteps=2, C=0.281, v_reset=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a layer of Quadratic Integrate-and-Fire (QIF) neurons.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>threshold</code></strong></dt>
<dd>Default: None.</dd>
<dt><strong><code>tau_rc</code></strong></dt>
<dd>Membrane time constant a.k.a. tau_m or tau in seconds. Default: 0.02.</dd>
<dt><strong><code>ts</code></strong></dt>
<dd>time-step value in seconds. Default: 0.001.</dd>
<dt>Cut-off threshold (negative-positive membrane potential update transition point). Default: None.</dt>
<dt><strong><code>a</code></strong></dt>
<dd>Sharpness parameter (upswing on the parabolic curve). Default: None.</dd>
<dt><strong><code>resting_potential</code></strong></dt>
<dd>Default: 0.0.</dd>
<dt><strong><code>refractory_timesteps</code></strong></dt>
<dd>Default: 2.</dd>
<dt><strong><code>C</code></strong></dt>
<dd>Capacitance. Default: 0.281.</dd>
<dt><strong><code>v_reset</code></strong></dt>
<dd>Default: None.</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;<code>u_c</code> being <code>None</code> will cause <code>u_c</code> to be <span><span class="MathJax_Preview">\frac{3}{4}V_{thresh}</span><script type="math/tex">\frac{3}{4}V_{thresh}</script></span>.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QIF(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001, u_c=None, a=0.001, resting_potential=0.0,
                 refractory_timesteps=2, C=0.281, v_reset=None):
        &#34;&#34;&#34;
        Creates a layer of Quadratic Integrate-and-Fire (QIF) neurons.
        Args:
            threshold: Default: None.
            tau_rc: Membrane time constant a.k.a. tau_m or tau in seconds. Default: 0.02.
            ts: time-step value in seconds. Default: 0.001.
            Cut-off threshold (negative-positive membrane potential update transition point). Default: None.
            a: Sharpness parameter (upswing on the parabolic curve). Default: None.
            resting_potential: Default: 0.0.
            refractory_timesteps: Default: 2.
            C: Capacitance. Default: 0.281.
            v_reset: Default: None.


        .. note:: `u_c` being `None` will cause `u_c` to be \(\\frac{3}{4}V_{thresh}\).
        &#34;&#34;&#34;
        Neuron.__init__(self, resting_potential=resting_potential, threshold=threshold)

        # assert abs(theta_rh / (resting_potential + delta_t)) &gt;= 10, \
        #     &#34;Needs to hold as it is assumed in Neuronal Dynamics book, Ch.5 Â¶ 5.2&#34;
        self.tau_rc = tau_rc
        self.ts = ts
        if u_c is None:  # guess from threshold
            u_c = -abs(threshold)*0.25 + threshold  # make the &#39;rheobase&#39; threshold smaller of the threshold by 25%
        self.u_c = u_c
        self.a = a
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self.refractory_periods = None
        self.C = C
        if v_reset is None:
            self.v_reset = resting_potential
        else:
            self.v_reset = v_reset

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None

    def __str__(self):
        return &#34;QIF_Neuron_rt&#34;+str(self.refractory_timesteps)+&#34;_C&#34;+str(self.C)

    def _f_ode(self, x, I=0):
        return self.a*(x - self.resting_potential)*(x-self.u_c) + (self.tau_rc/self.C)*I

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1):
        &#34;&#34;&#34;
        Calculates a (time-) step update for the neuron as specified by the following differential equation:
        $$
            \\tau_{rc}\\frac{du}{dt} = -a_0(u - u_{rest})(u - u_{c}) + R\\cdot I(t)
        $$
        Args:
            potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
            return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (bool): Default: 1.
        Returns:
            Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;

        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)

        previous_state = self.previous_state.clone().detach()

        # TODO should consider multiplying input by a resistance?
        # du = -(previous_state - self.resting_potential) \
        #      + self.delta_t * torch.exp((previous_state - self.theta_rh) / self.delta_t) \
        #      + potentials

        du = previous_state - self.resting_potential
        du = du * (previous_state - self.u_c) * self.a
        du /= self.tau_rc
        du += potentials/self.C
        du *= self.ts
        current_state = previous_state + du

        # inhibit where refractoriness is not consumed
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        torch.clip_(current_state, min=self.resting_potential)
        dudt = current_state - self.previous_state

        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0
        spikes = torch.div(thresholded.sign(), self.ts)
        # winners = sf.get_k_winners_davide(thresholded, spikes, self.per_neuron_thresh)
        winners = sf.get_k_winners(thresholded, kwta=n_winners, inhibition_radius=0, spikes=spikes)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            non_inihibited_spikes[0, w[0], :, :] = True
        current_state[spiked] = self.v_reset

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        self.previous_state = current_state

        # emitted spikes are scaled by dt
        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (current_state,)
        if return_dudt:
            ret += (dudt,)
        if return_winners:
            ret += (winners,)
        return ret</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.UniformQIF" href="#SpykeTorch.neurons.UniformQIF">UniformQIF</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="SpykeTorch.neurons.QIF.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    self.previous_state = None
    self.refractory_periods = None</code></pre>
</details>
</dd>
<dt id="SpykeTorch.neurons.QIF.__call__"><code class="name flex">
<span>def <span class="ident">__call__</span></span>(<span>self, potentials, return_thresholded_potentials=False, return_dudt=False, return_winners=True, n_winners=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates a (time-) step update for the neuron as specified by the following differential equation:
<span><span class="MathJax_Preview">
\tau_{rc}\frac{du}{dt} = -a_0(u - u_{rest})(u - u_{c}) + R\cdot I(t)
</span><script type="math/tex; mode=display">
\tau_{rc}\frac{du}{dt} = -a_0(u - u_{rest})(u - u_{c}) + R\cdot I(t)
</script></span></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>potentials</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.</dd>
<dt><strong><code>return_thresholded_potentials</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False</dd>
<dt><strong><code>return_dudt</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: False.</dd>
<dt><strong><code>return_winners</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: True.</dd>
<dt><strong><code>n_winners</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: 1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple</code></dt>
<dd>(spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
             return_winners=True, n_winners=1):
    &#34;&#34;&#34;
    Calculates a (time-) step update for the neuron as specified by the following differential equation:
    $$
        \\tau_{rc}\\frac{du}{dt} = -a_0(u - u_{rest})(u - u_{c}) + R\\cdot I(t)
    $$
    Args:
        potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
        return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
        return_dudt (bool): Default: False.
        return_winners (bool): Default: True.
        n_winners (bool): Default: 1.
    Returns:
        Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
    &#34;&#34;&#34;

    if self.previous_state is None:
        self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
        self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)

    previous_state = self.previous_state.clone().detach()

    # TODO should consider multiplying input by a resistance?
    # du = -(previous_state - self.resting_potential) \
    #      + self.delta_t * torch.exp((previous_state - self.theta_rh) / self.delta_t) \
    #      + potentials

    du = previous_state - self.resting_potential
    du = du * (previous_state - self.u_c) * self.a
    du /= self.tau_rc
    du += potentials/self.C
    du *= self.ts
    current_state = previous_state + du

    # inhibit where refractoriness is not consumed
    current_state[self.refractory_periods &gt; 0] = self.resting_potential
    torch.clip_(current_state, min=self.resting_potential)
    dudt = current_state - self.previous_state

    thresholded = self.get_thresholded_potentials(current_state)

    spiked = thresholded != 0.0
    spikes = torch.div(thresholded.sign(), self.ts)
    # winners = sf.get_k_winners_davide(thresholded, spikes, self.per_neuron_thresh)
    winners = sf.get_k_winners(thresholded, kwta=n_winners, inhibition_radius=0, spikes=spikes)
    non_inihibited_spikes = torch.full(spiked.shape, False)
    for w in winners:
        non_inihibited_spikes[0, w[0], :, :] = True
    current_state[spiked] = self.v_reset

    # update refractory periods
    self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
    self.refractory_periods[non_inihibited_spikes] = self.refractoriness

    self.previous_state = current_state

    # emitted spikes are scaled by dt
    ret = (spikes,)
    if return_thresholded_potentials:
        ret += (thresholded,)
    ret += (current_state,)
    if return_dudt:
        ret += (dudt,)
    if return_winners:
        ret += (winners,)
    return ret</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.Neuron.finalize_state_update" href="#SpykeTorch.neurons.Neuron.finalize_state_update">finalize_state_update</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.get_thresholded_potentials" href="#SpykeTorch.neurons.Neuron.get_thresholded_potentials">get_thresholded_potentials</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.plot_ode" href="#SpykeTorch.neurons.Neuron.plot_ode">plot_ode</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="SpykeTorch.neurons.Izhikevich"><code class="flex name class">
<span>class <span class="ident">Izhikevich</span></span>
<span>(</span><span>threshold, tau_rc=0.02, ts=0.001, a=0.02, b=0.2, c=0.0, d=8.0, resting_potential=0.0, refractory_timesteps=2, C=0.281, v_reset=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a layer of Izhikevich's neurons.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tau_rc</code></strong></dt>
<dd>Membrane time constant a.k.a. tau_m or tau in seconds. Default: 0.02.</dd>
<dt><strong><code>ts</code></strong></dt>
<dd>time-step value in seconds. Default: 0.001.</dd>
<dt><strong><code>delta_t</code></strong></dt>
<dd>Sharpness parameter (upswing on the exponential curve). If ~0, EIF behaves like LIF. Default: 0.5.</dd>
<dt><strong><code>theta_rh</code></strong></dt>
<dd>Rheobase threshold. Default: 5.</dd>
<dt><strong><code>resting_potential</code></strong></dt>
<dd>Default: 0.0.</dd>
<dt><strong><code>threshold</code></strong></dt>
<dd>Default: None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Izhikevich(Neuron):

    def __init__(self, threshold, tau_rc=0.02, ts=0.001,
                 a=0.02, b=0.2, c=0.0, d=8.0,
                 resting_potential=0.0,
                 refractory_timesteps=2, C=0.281, v_reset=None):
        &#34;&#34;&#34;
        Creates a layer of Izhikevich&#39;s neurons.

        Args:
            tau_rc: Membrane time constant a.k.a. tau_m or tau in seconds. Default: 0.02.
            ts: time-step value in seconds. Default: 0.001.
            delta_t: Sharpness parameter (upswing on the exponential curve). If ~0, EIF behaves like LIF. Default: 0.5.
            theta_rh: Rheobase threshold. Default: 5.
            resting_potential: Default: 0.0.
            threshold: Default: None.
        &#34;&#34;&#34;
        super(Izhikevich, self).__init__(resting_potential=resting_potential, threshold=threshold)

        # assert abs(theta_rh / (resting_potential + delta_t)) &gt;= 10, \
        #     &#34;Needs to hold as it is assumed in Neuronal Dynamics book, Ch.5 Â¶ 5.2&#34;
        self.tau_rc = tau_rc
        self.ts = ts
        # if theta_rh is None:  # guess from threshold
        #     theta_rh = -abs(threshold)*0.25 + threshold  # make the rheobase threshold smaller of the threshold by 25%
        # self.theta_rh = theta_rh
        self.refractory_timesteps = refractory_timesteps
        self.refractoriness = self.refractory_timesteps * ts
        self.refractory_periods = None
        self.C = C
        self.R = tau_rc/C
        self.a = a
        self.b = b
        self.d = d
        if v_reset is None:
            self.v_reset = resting_potential
        else:
            self.v_reset = v_reset
        self.c = c if c is not None else self.v_reset

    def reset(self):
        self.previous_state = None
        self.refractory_periods = None
        self.w = None

    def __str__(self):
        return &#34;Izhikevich_Neuron_rt&#34;+str(self.refractory_timesteps)+&#34;_C&#34;+str(self.C)

    def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
                 return_winners=True, n_winners=1):
        &#34;&#34;&#34;
        Calculates the update amount for the neuron as specified by the following differential equations:

        $$
            \\frac{du}{dt} = 0.04u^2 + 5u + 140 - \\omega + I \\\\
            \\frac{d\\omega}{dt} = a(b\\cdot u - \\omega)
        $$
        Args:
            potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
            return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
            return_dudt (bool): Default: False.
            return_winners (bool): Default: True.
            n_winners (bool): Default: 1.
        Returns:
            Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
        &#34;&#34;&#34;
        # potentials = torch.sum(potentials, (2, 3), keepdim=True)

        if self.previous_state is None:
            self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
            self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
            self.w = torch.full(potentials.size(), self.resting_potential*self.b, device=DEVICE)

        previous_state = self.previous_state.clone().detach()

        du = 0.04*previous_state**2 + 5*previous_state + 140
        du += potentials
        du -= self.w
        current_state = previous_state + du

        # inhibit where refractoriness is not consumed
        current_state[self.refractory_periods &gt; 0] = self.resting_potential
        # current_state.clamp_(self.resting_potential, None)

        dw = self.a*(self.b*current_state - self.w)
        dudt = current_state - self.previous_state
        # current_state.clip(self.resting_potential, None)

        thresholded = self.get_thresholded_potentials(current_state)

        spiked = thresholded != 0.0
        # Add a current jump only where there has been a spike
        self.w = self.w + dw
        self.w[spiked] += self.d

        # emitted spikes are scaled by dt
        spikes = torch.div(torch.abs(thresholded.sign()), self.ts)

        winners = sf.get_k_winners(thresholded, kwta=n_winners, inhibition_radius=0, spikes=spikes)
        non_inihibited_spikes = torch.full(spiked.shape, False)
        for w in winners:
            #non_inihibited_spikes[0, w[0], :, :] = True
            non_inihibited_spikes[0] = True  # TODO
        current_state[spiked] = self.v_reset

        # update refractory periods
        self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
        self.refractory_periods[non_inihibited_spikes] = self.refractoriness

        self.previous_state = current_state

        ret = (spikes,)
        if return_thresholded_potentials:
            ret += (thresholded,)
        ret += (current_state,)
        if return_dudt:
            ret += (dudt,)
        if return_winners:
            ret += (winners,)
        return ret</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="SpykeTorch.neurons.Izhikevich.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    self.previous_state = None
    self.refractory_periods = None
    self.w = None</code></pre>
</details>
</dd>
<dt id="SpykeTorch.neurons.Izhikevich.__call__"><code class="name flex">
<span>def <span class="ident">__call__</span></span>(<span>self, potentials, return_thresholded_potentials=False, return_dudt=False, return_winners=True, n_winners=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the update amount for the neuron as specified by the following differential equations:</p>
<p><span><span class="MathJax_Preview">
\frac{du}{dt} = 0.04u^2 + 5u + 140 - \omega + I \\
\frac{d\omega}{dt} = a(b\cdot u - \omega)
</span><script type="math/tex; mode=display">
\frac{du}{dt} = 0.04u^2 + 5u + 140 - \omega + I \\
\frac{d\omega}{dt} = a(b\cdot u - \omega)
</script></span></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>potentials</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.</dd>
<dt><strong><code>return_thresholded_potentials</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False</dd>
<dt><strong><code>return_dudt</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: False.</dd>
<dt><strong><code>return_winners</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: True.</dd>
<dt><strong><code>n_winners</code></strong> :&ensp;<code>bool</code></dt>
<dd>Default: 1.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple</code></dt>
<dd>(spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def __call__(self, potentials, return_thresholded_potentials=False, return_dudt=False,
             return_winners=True, n_winners=1):
    &#34;&#34;&#34;
    Calculates the update amount for the neuron as specified by the following differential equations:

    $$
        \\frac{du}{dt} = 0.04u^2 + 5u + 140 - \\omega + I \\\\
        \\frac{d\\omega}{dt} = a(b\\cdot u - \\omega)
    $$
    Args:
        potentials (Tensor): Input post-synaptic potentials. These are intended to be inside a torch.Tensor object and are the equivalent of the sum of the incoming spikes, each scaled by the strength of the synapse (convolution weights) they came through.
        return_thresholded_potentials (bool): If True, the tensor of thresholded potentials will be returned as well as the tensor of spike-wave. Default: False
        return_dudt (bool): Default: False.
        return_winners (bool): Default: True.
        n_winners (bool): Default: 1.
    Returns:
        Tuple: (spikes, [thresholded_potentials, ] current_state, [dudt, ] [winners, ])
    &#34;&#34;&#34;
    # potentials = torch.sum(potentials, (2, 3), keepdim=True)

    if self.previous_state is None:
        self.previous_state = torch.full(potentials.size(), self.resting_potential, device=DEVICE)
        self.refractory_periods = torch.full(potentials.size(), 0.0, device=DEVICE)
        self.w = torch.full(potentials.size(), self.resting_potential*self.b, device=DEVICE)

    previous_state = self.previous_state.clone().detach()

    du = 0.04*previous_state**2 + 5*previous_state + 140
    du += potentials
    du -= self.w
    current_state = previous_state + du

    # inhibit where refractoriness is not consumed
    current_state[self.refractory_periods &gt; 0] = self.resting_potential
    # current_state.clamp_(self.resting_potential, None)

    dw = self.a*(self.b*current_state - self.w)
    dudt = current_state - self.previous_state
    # current_state.clip(self.resting_potential, None)

    thresholded = self.get_thresholded_potentials(current_state)

    spiked = thresholded != 0.0
    # Add a current jump only where there has been a spike
    self.w = self.w + dw
    self.w[spiked] += self.d

    # emitted spikes are scaled by dt
    spikes = torch.div(torch.abs(thresholded.sign()), self.ts)

    winners = sf.get_k_winners(thresholded, kwta=n_winners, inhibition_radius=0, spikes=spikes)
    non_inihibited_spikes = torch.full(spiked.shape, False)
    for w in winners:
        #non_inihibited_spikes[0, w[0], :, :] = True
        non_inihibited_spikes[0] = True  # TODO
    current_state[spiked] = self.v_reset

    # update refractory periods
    self.refractory_periods[self.refractory_periods &gt; 0] -= self.ts
    self.refractory_periods[non_inihibited_spikes] = self.refractoriness

    self.previous_state = current_state

    ret = (spikes,)
    if return_thresholded_potentials:
        ret += (thresholded,)
    ret += (current_state,)
    if return_dudt:
        ret += (dudt,)
    if return_winners:
        ret += (winners,)
    return ret</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.Neuron.finalize_state_update" href="#SpykeTorch.neurons.Neuron.finalize_state_update">finalize_state_update</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.get_thresholded_potentials" href="#SpykeTorch.neurons.Neuron.get_thresholded_potentials">get_thresholded_potentials</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.plot_ode" href="#SpykeTorch.neurons.Neuron.plot_ode">plot_ode</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="SpykeTorch.neurons.HeterogeneousNeuron"><code class="flex name class">
<span>class <span class="ident">HeterogeneousNeuron</span></span>
<span>(</span><span>conv)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for layers of neurons having a non-homogeneous set of parameters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HeterogeneousNeuron(Neuron):

    def __init__(self, conv):
        &#34;&#34;&#34;
        Base class for layers of neurons having a non-homogeneous set of parameters.
        &#34;&#34;&#34;
        super().__init__(conv)

    def get_uniform_distribution(self, range, size):
        &#34;&#34;&#34;
        Creates a uniformly distributed set of values in the `range` and `size` provided.
        Args:
            range (list): Range to sample the values from.
            size (tuple): Size of the Tensor to sample.

        Returns:
            Tensor: Tensor containing the uniformly distributed values.
        &#34;&#34;&#34;
        ones = np.ones(size)
        uniform = np.random.uniform(*range, size=(size[0], size[1], 1, 1))
        uniform = torch.from_numpy(uniform*ones)
        return uniform.to(DEVICE)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.UniformEIF" href="#SpykeTorch.neurons.UniformEIF">UniformEIF</a></li>
<li><a title="SpykeTorch.neurons.UniformLIF" href="#SpykeTorch.neurons.UniformLIF">UniformLIF</a></li>
<li><a title="SpykeTorch.neurons.UniformQIF" href="#SpykeTorch.neurons.UniformQIF">UniformQIF</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="SpykeTorch.neurons.HeterogeneousNeuron.get_uniform_distribution"><code class="name flex">
<span>def <span class="ident">get_uniform_distribution</span></span>(<span>self, range, size)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a uniformly distributed set of values in the <code>range</code> and <code>size</code> provided.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>range</code></strong> :&ensp;<code>list</code></dt>
<dd>Range to sample the values from.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Size of the Tensor to sample.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>Tensor containing the uniformly distributed values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_uniform_distribution(self, range, size):
    &#34;&#34;&#34;
    Creates a uniformly distributed set of values in the `range` and `size` provided.
    Args:
        range (list): Range to sample the values from.
        size (tuple): Size of the Tensor to sample.

    Returns:
        Tensor: Tensor containing the uniformly distributed values.
    &#34;&#34;&#34;
    ones = np.ones(size)
    uniform = np.random.uniform(*range, size=(size[0], size[1], 1, 1))
    uniform = torch.from_numpy(uniform*ones)
    return uniform.to(DEVICE)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.Neuron.finalize_state_update" href="#SpykeTorch.neurons.Neuron.finalize_state_update">finalize_state_update</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.get_thresholded_potentials" href="#SpykeTorch.neurons.Neuron.get_thresholded_potentials">get_thresholded_potentials</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.plot_ode" href="#SpykeTorch.neurons.Neuron.plot_ode">plot_ode</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="SpykeTorch.neurons.UniformLIF"><code class="flex name class">
<span>class <span class="ident">UniformLIF</span></span>
<span>(</span><span>threshold, tau_range, ts=0.001, resting_potential=0.0, refractory_timesteps=2, C=0.281, per_neuron_thresh=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a layer of heterogeneous Leaky Integrate and Fire neuron(s).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>threshold</code></strong></dt>
<dd>threshold above which the neuron(s) fires a spike.</dd>
<dt><strong><code>tau_range</code></strong> :&ensp;<code>list</code></dt>
<dd>Range of values from which to sample the <span><span class="MathJax_Preview">\tau_{rc}</span><script type="math/tex">\tau_{rc}</script></span>.</dd>
<dt><strong><code>ts</code></strong></dt>
<dd>the time step used for computations, needs to be at least 10 times smaller than tau_rc.</dd>
<dt><strong><code>resting_potential</code></strong></dt>
<dd>potential at which the neuron(s) is set to after a spike.</dd>
<dt><strong><code>refractory_timesteps</code></strong></dt>
<dd>number of timestep of hyperpolarization after a spike.</dd>
<dt><strong><code>C</code></strong></dt>
<dd>Capacitance of the membrane potential. Influences the input potential effect.</dd>
<dt><strong><code>per_neuron_thresh</code></strong></dt>
<dd>defines neuron-wise threshold. If None, a layer-wise threshold is used. Default: None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UniformLIF(LIF, HeterogeneousNeuron):

    def __init__(self, threshold, tau_range, ts=0.001, resting_potential=0.0, refractory_timesteps=2, C=0.281,
                 per_neuron_thresh=None):
        &#34;&#34;&#34;
        Creates a layer of heterogeneous Leaky Integrate and Fire neuron(s).

        Args:
            threshold: threshold above which the neuron(s) fires a spike.
            tau_range (list): Range of values from which to sample the \(\\tau_{rc}\).
            ts: the time step used for computations, needs to be at least 10 times smaller than tau_rc.
            resting_potential: potential at which the neuron(s) is set to after a spike.
            refractory_timesteps: number of timestep of hyperpolarization after a spike.
            C: Capacitance of the membrane potential. Influences the input potential effect.
            per_neuron_thresh: defines neuron-wise threshold. If None, a layer-wise threshold is used. Default: None.
        &#34;&#34;&#34;
        LIF.__init__(self, threshold, C=C, refractory_timesteps=refractory_timesteps, ts=ts,
                     resting_potential=resting_potential, per_neuron_thresh=per_neuron_thresh)
        self.threshold = threshold
        self.tau_range = tau_range
        self.taus = None

    def __call__(self, potentials, *args, **kwargs):

        if self.taus is None:
            self.taus = self.get_uniform_distribution(self.tau_range, potentials.shape)
            self.ts_over_tau = self.ts / self.taus.cpu().numpy()  # for better performance (compute once and for all)
            self.exp_term = np.exp(-self.ts_over_tau)  # for better performance (compute once and for all)

        return super(UniformLIF, self).__call__(potentials, *args, **kwargs)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.LIF" href="#SpykeTorch.neurons.LIF">LIF</a></li>
<li><a title="SpykeTorch.neurons.HeterogeneousNeuron" href="#SpykeTorch.neurons.HeterogeneousNeuron">HeterogeneousNeuron</a></li>
<li><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="SpykeTorch.neurons.LIF" href="#SpykeTorch.neurons.LIF">LIF</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.LIF.__call__" href="#SpykeTorch.neurons.LIF.__call__">__call__</a></code></li>
<li><code><a title="SpykeTorch.neurons.LIF.finalize_state_update" href="#SpykeTorch.neurons.Neuron.finalize_state_update">finalize_state_update</a></code></li>
<li><code><a title="SpykeTorch.neurons.LIF.get_thresholded_potentials" href="#SpykeTorch.neurons.Neuron.get_thresholded_potentials">get_thresholded_potentials</a></code></li>
<li><code><a title="SpykeTorch.neurons.LIF.plot_ode" href="#SpykeTorch.neurons.Neuron.plot_ode">plot_ode</a></code></li>
</ul>
</li>
<li><code><b><a title="SpykeTorch.neurons.HeterogeneousNeuron" href="#SpykeTorch.neurons.HeterogeneousNeuron">HeterogeneousNeuron</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.HeterogeneousNeuron.get_uniform_distribution" href="#SpykeTorch.neurons.HeterogeneousNeuron.get_uniform_distribution">get_uniform_distribution</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="SpykeTorch.neurons.UniformEIF"><code class="flex name class">
<span>class <span class="ident">UniformEIF</span></span>
<span>(</span><span>threshold, tau_range, ts=0.001, delta_t=0.5, theta_rh=None, resting_potential=0.0, refractory_timesteps=2, C=0.281, v_reset=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a layer of heterogeneous Exponential Integrate and Fire (EIF) neurons.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>threshold</code></strong></dt>
<dd>Default: None.</dd>
<dt><strong><code>tau_range</code></strong> :&ensp;<code>list</code></dt>
<dd>Range of values from which to sample the <span><span class="MathJax_Preview">\tau_{rc}</span><script type="math/tex">\tau_{rc}</script></span>.</dd>
<dt><strong><code>ts</code></strong></dt>
<dd>time-step value in seconds. Default: 0.001.</dd>
<dt><strong><code>delta_t</code></strong></dt>
<dd>Sharpness parameter (upswing on the exponential curve). If ~0, EIF behaves like LIF. Default: 0.5.</dd>
<dt><strong><code>theta_rh</code></strong></dt>
<dd>Rheobase threshold. Default: None.</dd>
<dt><strong><code>resting_potential</code></strong></dt>
<dd>Default: 0.0.</dd>
<dt><strong><code>refractory_timesteps</code></strong></dt>
<dd>Default: 2.</dd>
<dt><strong><code>C</code></strong></dt>
<dd>Capacitance. Default: 0.281.</dd>
<dt><strong><code>v_reset</code></strong></dt>
<dd>Default: None.</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;<code>theta_rh</code> being <code>None</code> will cause <code>theta_rh</code> to be <span><span class="MathJax_Preview">\frac{3}{4}V_{thresh}</span><script type="math/tex">\frac{3}{4}V_{thresh}</script></span>.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UniformEIF(EIF, HeterogeneousNeuron):

    def __init__(self, threshold, tau_range, ts=0.001, delta_t=0.5, theta_rh=None, resting_potential=0.0,
                 refractory_timesteps=2, C=0.281, v_reset=None):
        &#34;&#34;&#34;
        Creates a layer of heterogeneous Exponential Integrate and Fire (EIF) neurons.

        Args:
            threshold: Default: None.
            tau_range (list): Range of values from which to sample the \(\\tau_{rc}\).
            ts: time-step value in seconds. Default: 0.001.
            delta_t: Sharpness parameter (upswing on the exponential curve). If ~0, EIF behaves like LIF. Default: 0.5.
            theta_rh: Rheobase threshold. Default: None.
            resting_potential: Default: 0.0.
            refractory_timesteps: Default: 2.
            C: Capacitance. Default: 0.281.
            v_reset: Default: None.

        .. note:: `theta_rh` being `None` will cause `theta_rh` to be \(\\frac{3}{4}V_{thresh}\).
        &#34;&#34;&#34;
        EIF.__init__(self, threshold=threshold, tau_rc=0.02, ts=ts, delta_t=delta_t, theta_rh=theta_rh,
                     resting_potential=resting_potential, refractory_timesteps=refractory_timesteps, C=C,
                     v_reset=v_reset)

        HeterogeneousNeuron.__init__(self)
        self.tau_range = tau_range
        self.taus = None

    def __call__(self, potentials, *args, **kwargs):

        if self.taus is None:
            self.taus = self.get_uniform_distribution(self.tau_range, potentials.shape)

        return super(UniformEIF, self).__call__(potentials, *args, **kwargs)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.EIF" href="#SpykeTorch.neurons.EIF">EIF</a></li>
<li><a title="SpykeTorch.neurons.HeterogeneousNeuron" href="#SpykeTorch.neurons.HeterogeneousNeuron">HeterogeneousNeuron</a></li>
<li><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="SpykeTorch.neurons.EIF" href="#SpykeTorch.neurons.EIF">EIF</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.EIF.__call__" href="#SpykeTorch.neurons.EIF.__call__">__call__</a></code></li>
<li><code><a title="SpykeTorch.neurons.EIF.finalize_state_update" href="#SpykeTorch.neurons.Neuron.finalize_state_update">finalize_state_update</a></code></li>
<li><code><a title="SpykeTorch.neurons.EIF.get_thresholded_potentials" href="#SpykeTorch.neurons.Neuron.get_thresholded_potentials">get_thresholded_potentials</a></code></li>
<li><code><a title="SpykeTorch.neurons.EIF.plot_ode" href="#SpykeTorch.neurons.Neuron.plot_ode">plot_ode</a></code></li>
</ul>
</li>
<li><code><b><a title="SpykeTorch.neurons.HeterogeneousNeuron" href="#SpykeTorch.neurons.HeterogeneousNeuron">HeterogeneousNeuron</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.HeterogeneousNeuron.get_uniform_distribution" href="#SpykeTorch.neurons.HeterogeneousNeuron.get_uniform_distribution">get_uniform_distribution</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="SpykeTorch.neurons.UniformQIF"><code class="flex name class">
<span>class <span class="ident">UniformQIF</span></span>
<span>(</span><span>threshold, tau_range, ts=0.001, u_c=None, a=0.001, resting_potential=0.0, refractory_timesteps=2, C=0.281, v_reset=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a layer of heterogeneous Quadratic Integrate-and-Fire (QIF) neurons.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>threshold</code></strong></dt>
<dd>Default: None.</dd>
<dt><strong><code>tau_range</code></strong> :&ensp;<code>list</code></dt>
<dd>Range of values from which to sample the <span><span class="MathJax_Preview">\tau_{rc}</span><script type="math/tex">\tau_{rc}</script></span>.</dd>
<dt><strong><code>ts</code></strong></dt>
<dd>time-step value in seconds. Default: 0.001.</dd>
<dt><strong><code>u_c</code></strong></dt>
<dd>Cut-off threshold (negative-positive membrane potential update transition point). Default: 5.</dd>
<dt><strong><code>a</code></strong></dt>
<dd>Sharpness parameter (upswing on the parabolic curve). Default: None.</dd>
<dt><strong><code>resting_potential</code></strong></dt>
<dd>Default: 0.0.</dd>
<dt><strong><code>refractory_timesteps</code></strong></dt>
<dd>Default: 2.</dd>
<dt><strong><code>C</code></strong></dt>
<dd>Capacitance. Default: 0.281.</dd>
<dt><strong><code>v_reset</code></strong></dt>
<dd>Default: None.</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note:&ensp;<code>u_c</code> being <code>None</code> will cause <code>u_c</code> to be <span><span class="MathJax_Preview">\frac{3}{4}V_{thresh}</span><script type="math/tex">\frac{3}{4}V_{thresh}</script></span>.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class UniformQIF(QIF, HeterogeneousNeuron):

    def __init__(self, threshold, tau_range, ts=0.001, u_c=None, a=0.001, resting_potential=0.0,
                 refractory_timesteps=2, C=0.281, v_reset=None):
        &#34;&#34;&#34;
        Creates a layer of heterogeneous Quadratic Integrate-and-Fire (QIF) neurons.
        Args:
            threshold: Default: None.
            tau_range (list): Range of values from which to sample the \(\\tau_{rc}\).
            ts: time-step value in seconds. Default: 0.001.
            u_c: Cut-off threshold (negative-positive membrane potential update transition point). Default: 5.
            a: Sharpness parameter (upswing on the parabolic curve). Default: None.
            resting_potential: Default: 0.0.
            refractory_timesteps: Default: 2.
            C: Capacitance. Default: 0.281.
            v_reset: Default: None.

        .. note:: `u_c` being `None` will cause `u_c` to be \(\\frac{3}{4}V_{thresh}\).
        &#34;&#34;&#34;
        QIF.__init__(self, threshold, tau_rc=0.02, ts=ts, u_c=u_c, a=a, resting_potential=resting_potential,
                 refractory_timesteps=refractory_timesteps, C=C, v_reset=v_reset)
        HeterogeneousNeuron.__init__(self)
        self.tau_range = tau_range
        self.taus = None

    def __call__(self, potentials, *args, **kwargs):
        if self.taus is None:
            self.taus = self.get_uniform_distribution(self.tau_range, potentials.shape)

        return super(UniformQIF, self).__call__(potentials, *args, **kwargs)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="SpykeTorch.neurons.QIF" href="#SpykeTorch.neurons.QIF">QIF</a></li>
<li><a title="SpykeTorch.neurons.HeterogeneousNeuron" href="#SpykeTorch.neurons.HeterogeneousNeuron">HeterogeneousNeuron</a></li>
<li><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="SpykeTorch.neurons.QIF" href="#SpykeTorch.neurons.QIF">QIF</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.QIF.__call__" href="#SpykeTorch.neurons.QIF.__call__">__call__</a></code></li>
<li><code><a title="SpykeTorch.neurons.QIF.finalize_state_update" href="#SpykeTorch.neurons.Neuron.finalize_state_update">finalize_state_update</a></code></li>
<li><code><a title="SpykeTorch.neurons.QIF.get_thresholded_potentials" href="#SpykeTorch.neurons.Neuron.get_thresholded_potentials">get_thresholded_potentials</a></code></li>
<li><code><a title="SpykeTorch.neurons.QIF.plot_ode" href="#SpykeTorch.neurons.Neuron.plot_ode">plot_ode</a></code></li>
</ul>
</li>
<li><code><b><a title="SpykeTorch.neurons.HeterogeneousNeuron" href="#SpykeTorch.neurons.HeterogeneousNeuron">HeterogeneousNeuron</a></b></code>:
<ul class="hlist">
<li><code><a title="SpykeTorch.neurons.HeterogeneousNeuron.get_uniform_distribution" href="#SpykeTorch.neurons.HeterogeneousNeuron.get_uniform_distribution">get_uniform_distribution</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="SpykeTorch" href="index.html">SpykeTorch</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="SpykeTorch.neurons.Neuron" href="#SpykeTorch.neurons.Neuron">Neuron</a></code></h4>
<ul class="">
<li><code><a title="SpykeTorch.neurons.Neuron.reset" href="#SpykeTorch.neurons.Neuron.reset">reset</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.get_params" href="#SpykeTorch.neurons.Neuron.get_params">get_params</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.get_thresholded_potentials" href="#SpykeTorch.neurons.Neuron.get_thresholded_potentials">get_thresholded_potentials</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.plot_ode" href="#SpykeTorch.neurons.Neuron.plot_ode">plot_ode</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.finalize_state_update" href="#SpykeTorch.neurons.Neuron.finalize_state_update">finalize_state_update</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.threshold" href="#SpykeTorch.neurons.Neuron.threshold">threshold</a></code></li>
<li><code><a title="SpykeTorch.neurons.Neuron.per_neuron_thresh" href="#SpykeTorch.neurons.Neuron.per_neuron_thresh">per_neuron_thresh</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SpykeTorch.neurons.IF" href="#SpykeTorch.neurons.IF">IF</a></code></h4>
<ul class="">
<li><code><a title="SpykeTorch.neurons.IF.reset" href="#SpykeTorch.neurons.IF.reset">reset</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SpykeTorch.neurons.LIF" href="#SpykeTorch.neurons.LIF">LIF</a></code></h4>
<ul class="">
<li><code><a title="SpykeTorch.neurons.LIF.reset" href="#SpykeTorch.neurons.LIF.reset">reset</a></code></li>
<li><code><a title="SpykeTorch.neurons.LIF.__call__" href="#SpykeTorch.neurons.LIF.__call__">__call__</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SpykeTorch.neurons.LIF_Simple" href="#SpykeTorch.neurons.LIF_Simple">LIF_Simple</a></code></h4>
<ul class="">
<li><code><a title="SpykeTorch.neurons.LIF_Simple.reset" href="#SpykeTorch.neurons.LIF_Simple.reset">reset</a></code></li>
<li><code><a title="SpykeTorch.neurons.LIF_Simple.__call__" href="#SpykeTorch.neurons.LIF_Simple.__call__">__call__</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SpykeTorch.neurons.LIF_ode" href="#SpykeTorch.neurons.LIF_ode">LIF_ode</a></code></h4>
<ul class="">
<li><code><a title="SpykeTorch.neurons.LIF_ode.reset" href="#SpykeTorch.neurons.LIF_ode.reset">reset</a></code></li>
<li><code><a title="SpykeTorch.neurons.LIF_ode.__call__" href="#SpykeTorch.neurons.LIF_ode.__call__">__call__</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SpykeTorch.neurons.EIF" href="#SpykeTorch.neurons.EIF">EIF</a></code></h4>
<ul class="">
<li><code><a title="SpykeTorch.neurons.EIF.reset" href="#SpykeTorch.neurons.EIF.reset">reset</a></code></li>
<li><code><a title="SpykeTorch.neurons.EIF.__call__" href="#SpykeTorch.neurons.EIF.__call__">__call__</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SpykeTorch.neurons.EIF_Simple" href="#SpykeTorch.neurons.EIF_Simple">EIF_Simple</a></code></h4>
<ul class="">
<li><code><a title="SpykeTorch.neurons.EIF_Simple.reset" href="#SpykeTorch.neurons.EIF_Simple.reset">reset</a></code></li>
<li><code><a title="SpykeTorch.neurons.EIF_Simple.per_neuron_thresh" href="#SpykeTorch.neurons.EIF_Simple.per_neuron_thresh">per_neuron_thresh</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SpykeTorch.neurons.AdEx" href="#SpykeTorch.neurons.AdEx">AdEx</a></code></h4>
<ul class="">
<li><code><a title="SpykeTorch.neurons.AdEx.reset" href="#SpykeTorch.neurons.AdEx.reset">reset</a></code></li>
<li><code><a title="SpykeTorch.neurons.AdEx.__call__" href="#SpykeTorch.neurons.AdEx.__call__">__call__</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SpykeTorch.neurons.QIF" href="#SpykeTorch.neurons.QIF">QIF</a></code></h4>
<ul class="">
<li><code><a title="SpykeTorch.neurons.QIF.reset" href="#SpykeTorch.neurons.QIF.reset">reset</a></code></li>
<li><code><a title="SpykeTorch.neurons.QIF.__call__" href="#SpykeTorch.neurons.QIF.__call__">__call__</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SpykeTorch.neurons.Izhikevich" href="#SpykeTorch.neurons.Izhikevich">Izhikevich</a></code></h4>
<ul class="">
<li><code><a title="SpykeTorch.neurons.Izhikevich.reset" href="#SpykeTorch.neurons.Izhikevich.reset">reset</a></code></li>
<li><code><a title="SpykeTorch.neurons.Izhikevich.__call__" href="#SpykeTorch.neurons.Izhikevich.__call__">__call__</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SpykeTorch.neurons.HeterogeneousNeuron" href="#SpykeTorch.neurons.HeterogeneousNeuron">HeterogeneousNeuron</a></code></h4>
<ul class="">
<li><code><a title="SpykeTorch.neurons.HeterogeneousNeuron.get_uniform_distribution" href="#SpykeTorch.neurons.HeterogeneousNeuron.get_uniform_distribution">get_uniform_distribution</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="SpykeTorch.neurons.UniformLIF" href="#SpykeTorch.neurons.UniformLIF">UniformLIF</a></code></h4>
</li>
<li>
<h4><code><a title="SpykeTorch.neurons.UniformEIF" href="#SpykeTorch.neurons.UniformEIF">UniformEIF</a></code></h4>
</li>
<li>
<h4><code><a title="SpykeTorch.neurons.UniformQIF" href="#SpykeTorch.neurons.UniformQIF">UniformQIF</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>
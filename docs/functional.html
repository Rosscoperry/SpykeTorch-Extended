<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>SpykeTorch.functional API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>SpykeTorch.functional</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import torch
import torch.nn as nn
import torch.nn.functional as fn
import numpy as np
from .utils import to_pair

# padding
# pad = (padLeft, padRight, padTop, padBottom)
def pad(input, pad, value=0):
    r&#34;&#34;&#34;Applies 2D padding on the input tensor.

    Args:
        input (Tensor): The input tensor.
        pad (tuple): A tuple of 4 integers in the form of (padLeft, padRight, padTop, padBottom)
        value (int or float): The value of padding. Default: 0

    Returns:
        Tensor: Padded tensor.
    &#34;&#34;&#34;
    return fn.pad(input, pad, value=value)

# pooling
def pooling(input, kernel_size, stride=None, padding=0):
    r&#34;&#34;&#34;Performs a 2D max-pooling over an input signal (spike-wave or potentials) composed of several input
    planes.

    Args:
        input (Tensor): The input tensor.
        kernel_size (int or tuple): Size of the pooling window.
        stride (int or tuple, optional): Stride of the pooling window. Default: None
        padding (int or tuple, optional): Size of the padding. Default: 0

    Returns:
        Tensor: The result of the max-pooling operation.
    &#34;&#34;&#34;
    return fn.max_pool2d(input, kernel_size, stride, padding)

def fire(potentials, threshold=None, return_thresholded_potentials=False):
    r&#34;&#34;&#34;Computes the spike-wave tensor from tensor of potentials. If :attr:`threshold` is :attr:`None`, all the neurons
    emit one spike (if the potential is greater than zero) in the last time step.

    Args:
        potentials (Tensor): The tensor of input potentials.
        threshold (float): Firing threshold. Default: None
        return_thresholded_potentials (boolean): If True, the tensor of thresholded potentials will be returned
        as well as the tensor of spike-wave. Default: False

    Returns:
        Tensor: Spike-wave tensor.
    &#34;&#34;&#34;
    thresholded = potentials.clone().detach()
    if threshold is None:
        thresholded[:-1]=0
    else:
        fn.threshold_(thresholded, threshold, 0)
    if return_thresholded_potentials:
        return thresholded.sign(), thresholded
    return thresholded.sign()

def fire_(potentials, threshold=None):
    r&#34;&#34;&#34;The inplace version of :func:`~fire`
    &#34;&#34;&#34;
    if threshold is None:
        potentials[:-1]=0
    else:
        fn.threshold_(potentials, threshold, 0)
    potentials.sign_()

def threshold(potentials, threshold=None):
    r&#34;&#34;&#34;Applies a threshold on potentials by which all of the values lower or equal to the threshold becomes zero.
    If :attr:`threshold` is :attr:`None`, only the potentials corresponding to the final time step will survive.

    Args:
        potentials (Tensor): The tensor of input potentials.
        threshold (float): The threshold value. Default: None

    Returns:
        Tensor: Thresholded potentials.
    &#34;&#34;&#34;
    outputs = potentials.clone().detach()
    if threshold is None:
        outputs[:-1]=0
    else:
        fn.threshold_(outputs, threshold, 0)
    return outputs

def threshold_(potentials, threshold=None):
    r&#34;&#34;&#34;The inplace version of :func:`~threshold`
    &#34;&#34;&#34;
    if threshold is None:
        potentials[:-1]=0
    else:
        fn.threshold_(potentials, threshold, 0)

# in each position, the most fitted feature will survive (first earliest spike then maximum potential)
# it is assumed that the threshold function is applied on the input potentials
def pointwise_inhibition(thresholded_potentials):
    r&#34;&#34;&#34;Performs point-wise inhibition between feature maps. After inhibition, at most one neuron is allowed to fire at each
    position, which is the neuron with the earliest spike time. If the spike times are the same, the neuron with the maximum
    potential will be chosen. As a result, the potential of all of the inhibited neurons will be reset to zero.

    Args:
        thresholded_potentials (Tensor): The tensor of thresholded input potentials.

    Returns:
        Tensor: Inhibited potentials.
    &#34;&#34;&#34;
    # maximum of each position in each time step
    maximum = torch.max(thresholded_potentials, dim=1, keepdim=True)
    # compute signs for detection of the earliest spike
    clamp_pot = maximum[0].sign()
    # maximum of clamped values is the indices of the earliest spikes
    clamp_pot_max_1 = (clamp_pot.size(0) - clamp_pot.sum(dim = 0, keepdim=True)).long()
    clamp_pot_max_1.clamp_(0,clamp_pot.size(0)-1)
    clamp_pot_max_0 = clamp_pot[-1:,:,:,:]
    # finding winners (maximum potentials between early spikes)
    winners = maximum[1].gather(0, clamp_pot_max_1)
    # generating inhibition coefficient
    coef = torch.zeros_like(thresholded_potentials[0]).unsqueeze_(0)
    coef.scatter_(1, winners,clamp_pot_max_0)
    # applying inhibition to potentials (broadcasting multiplication)
    return torch.mul(thresholded_potentials, coef)

# inhibiting particular features, preventing them to be winners
# inhibited_features is a list of features numbers to be inhibited
def feature_inhibition_(potentials, inhibited_features):
    r&#34;&#34;&#34;The inplace version of :func:`~feature_inhibition`
    &#34;&#34;&#34;
    if len(inhibited_features) != 0:
        potentials[:, inhibited_features, :, :] = 0

def feature_inhibition(potentials, inhibited_features):
    r&#34;&#34;&#34;Inhibits specified features (reset the corresponding neurons&#39; potentials to zero).

    Args:
        potentials (Tensor): The tensor of input potentials.
        inhibited_features (List): The list of features to be inhibited.

    Returns:
        Tensor: Inhibited potentials.
    &#34;&#34;&#34;
    potentials_copy = potentials.clone().detach()
    if len(inhibited_features) != 0:
        feature_inhibition_(potentials_copy, inhibited_features)
    return potentials_copy

# returns list of winners
# inhibition_radius is to increase the chance of diversity among features (if needed)
def get_k_winners(potentials, kwta = 1, inhibition_radius = 0, spikes = None):
    r&#34;&#34;&#34;Finds at most :attr:`kwta` winners first based on the earliest spike time, then based on the maximum potential.
    It returns a list of winners, each in a tuple of form (feature, row, column).

    .. note::

        Winners are selected sequentially. Each winner inhibits surrounding neruons in a specific radius in all of the
        other feature maps. Note that only one winner can be selected from each feature map.

    Args:
        potentials (Tensor): The tensor of input potentials.
        kwta (int, optional): The number of winners. Default: 1
        inhibition_radius (int, optional): The radius of lateral inhibition. Default: 0
        spikes (Tensor, optional): Spike-wave corresponding to the input potentials. Default: None

    Returns:
        List: List of winners.
    &#34;&#34;&#34;
    if spikes is None:
        spikes = potentials.sign()
    # finding earliest potentials for each position in each feature
    maximum = (spikes.size(0) - spikes.sum(dim = 0, keepdim=True)).long()
    maximum.clamp_(0,spikes.size(0)-1)
    values = potentials.gather(dim=0, index=maximum) # gathering values
    # propagating the earliest potential through the whole timesteps
    truncated_pot = spikes * values
    # summation with a high enough value (maximum of potential summation over timesteps) at spike positions
    v = truncated_pot.max() * potentials.size(0)
    truncated_pot.addcmul_(spikes,v)
    # summation over all timesteps
    total = truncated_pot.sum(dim=0,keepdim=True)
    
    total.squeeze_(0)
    global_pooling_size = tuple(total.size())
    winners = []
    for k in range(kwta):
        max_val,max_idx = total.view(-1).max(0)
        if max_val.item() != 0:
            # finding the 3d position of the maximum value
            max_idx_unraveled = np.unravel_index(max_idx.item(),global_pooling_size)
            # adding to the winners list
            winners.append(max_idx_unraveled)
            # preventing the same feature to be the next winner
            total[max_idx_unraveled[0],:,:] = 0
            # columnar inhibition (increasing the chance of leanring diverse features)
            if inhibition_radius != 0:
                rowMin,rowMax = max(0,max_idx_unraveled[-2]-inhibition_radius),min(total.size(-2),max_idx_unraveled[-2]+inhibition_radius+1)
                colMin,colMax = max(0,max_idx_unraveled[-1]-inhibition_radius),min(total.size(-1),max_idx_unraveled[-1]+inhibition_radius+1)
                total[:,rowMin:rowMax,colMin:colMax] = 0
        else:
            break
    return winners

# decrease lateral intencities by factors given in the inhibition_kernel
def intensity_lateral_inhibition(intencities, inhibition_kernel):
    r&#34;&#34;&#34;Applies lateral inhibition on intensities. For each location, this inhibition decreases the intensity of the
    surrounding cells that has lower intensities by a specific factor. This factor is relative to the distance of the
    neighbors and are put in the :attr:`inhibition_kernel`.

    Args:
        intencities (Tensor): The tensor of input intensities.
        inhibition_kernel (Tensor): The tensor of inhibition factors.

    Returns:
        Tensor: Inhibited intensities.
    &#34;&#34;&#34;
    intencities.squeeze_(0)
    intencities.unsqueeze_(1)

    inh_win_size = inhibition_kernel.size(-1)
    rad = inh_win_size//2
    # repeat each value
    values = intencities.reshape(intencities.size(0),intencities.size(1),-1,1)
    values = values.repeat(1,1,1,inh_win_size)
    values = values.reshape(intencities.size(0),intencities.size(1),-1,intencities.size(-1)*inh_win_size)
    values = values.repeat(1,1,1,inh_win_size)
    values = values.reshape(intencities.size(0),intencities.size(1),-1,intencities.size(-1)*inh_win_size)
    # extend patches
    padded = fn.pad(intencities,(rad,rad,rad,rad))
    # column-wise
    patches = padded.unfold(-1,inh_win_size,1)
    patches = patches.reshape(patches.size(0),patches.size(1),patches.size(2),-1,patches.size(3)*patches.size(4))
    patches.squeeze_(-2)
    # row-wise
    patches = patches.unfold(-2,inh_win_size,1).transpose(-1,-2)
    patches = patches.reshape(patches.size(0),patches.size(1),1,-1,patches.size(-1))
    patches.squeeze_(-3)
    # compare each element by its neighbors
    coef = values - patches
    coef.clamp_(min=0).sign_() # &#34;ones&#34; are neighbors greater than center
    # convolution with full stride to get accumulative inhibiiton factor
    factors = fn.conv2d(coef, inhibition_kernel, stride=inh_win_size)
    result = intencities + intencities * factors

    intencities.squeeze_(1)
    intencities.unsqueeze_(0)
    result.squeeze_(1)
    result.unsqueeze_(0)
    return result

# performs local normalization
# on each region (of size radius*2 + 1) the mean value is computed and 
# intensities will be divided by the mean value
# x is a 4D tensor
def local_normalization(input, normalization_radius, eps=1e-12):
    r&#34;&#34;&#34;Applies local normalization. on each region (of size radius*2 + 1) the mean value is computed and the
    intensities will be divided by the mean value. The input is a 4D tensor.

    Args:
        input (Tensor): The input tensor of shape (timesteps, features, height, width).
        normalization_radius (int): The radius of normalization window.

    Returns:
        Tensor: Locally normalized tensor.
    &#34;&#34;&#34;
    # computing local mean by 2d convolution
    kernel = torch.ones(1,1,normalization_radius*2+1,normalization_radius*2+1,device=input.device).float()/((normalization_radius*2+1)**2)
    # rearrange 4D tensor so input channels will be considered as minibatches
    y = input.squeeze(0) # removes minibatch dim which was 1
    y.unsqueeze_(1)  # adds a dimension after channels so previous channels are now minibatches
    means = fn.conv2d(y,kernel,padding=normalization_radius) + eps # computes means
    y = y/means # normalization
    # swap minibatch with channels
    y.squeeze_(1)
    y.unsqueeze_(0)
    return y</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="SpykeTorch.functional.pad"><code class="name flex">
<span>def <span class="ident">pad</span></span>(<span>input, pad, value=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies 2D padding on the input tensor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>The input tensor.</dd>
<dt><strong><code>pad</code></strong> :&ensp;<code>tuple</code></dt>
<dd>A tuple of 4 integers in the form of (padLeft, padRight, padTop, padBottom)</dd>
<dt><strong><code>value</code></strong> :&ensp;<code>int</code> or <code>float</code></dt>
<dd>The value of padding. Default: 0</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>Padded tensor.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pad(input, pad, value=0):
    r&#34;&#34;&#34;Applies 2D padding on the input tensor.

    Args:
        input (Tensor): The input tensor.
        pad (tuple): A tuple of 4 integers in the form of (padLeft, padRight, padTop, padBottom)
        value (int or float): The value of padding. Default: 0

    Returns:
        Tensor: Padded tensor.
    &#34;&#34;&#34;
    return fn.pad(input, pad, value=value)</code></pre>
</details>
</dd>
<dt id="SpykeTorch.functional.pooling"><code class="name flex">
<span>def <span class="ident">pooling</span></span>(<span>input, kernel_size, stride=None, padding=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs a 2D max-pooling over an input signal (spike-wave or potentials) composed of several input
planes.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>The input tensor.</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int</code> or <code>tuple</code></dt>
<dd>Size of the pooling window.</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code> or <code>tuple</code>, optional</dt>
<dd>Stride of the pooling window. Default: None</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>int</code> or <code>tuple</code>, optional</dt>
<dd>Size of the padding. Default: 0</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>The result of the max-pooling operation.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pooling(input, kernel_size, stride=None, padding=0):
    r&#34;&#34;&#34;Performs a 2D max-pooling over an input signal (spike-wave or potentials) composed of several input
    planes.

    Args:
        input (Tensor): The input tensor.
        kernel_size (int or tuple): Size of the pooling window.
        stride (int or tuple, optional): Stride of the pooling window. Default: None
        padding (int or tuple, optional): Size of the padding. Default: 0

    Returns:
        Tensor: The result of the max-pooling operation.
    &#34;&#34;&#34;
    return fn.max_pool2d(input, kernel_size, stride, padding)</code></pre>
</details>
</dd>
<dt id="SpykeTorch.functional.fire"><code class="name flex">
<span>def <span class="ident">fire</span></span>(<span>potentials, threshold=None, return_thresholded_potentials=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes the spike-wave tensor from tensor of potentials. If :attr:<code><a title="SpykeTorch.functional.threshold" href="#SpykeTorch.functional.threshold">threshold()</a></code> is :attr:<code>None</code>, all the neurons
emit one spike (if the potential is greater than zero) in the last time step.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>potentials</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>The tensor of input potentials.</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>Firing threshold. Default: None</dd>
<dt><strong><code>return_thresholded_potentials</code></strong> :&ensp;<code>boolean</code></dt>
<dd>If True, the tensor of thresholded potentials will be returned</dd>
</dl>
<p>as well as the tensor of spike-wave. Default: False</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>Spike-wave tensor.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fire(potentials, threshold=None, return_thresholded_potentials=False):
    r&#34;&#34;&#34;Computes the spike-wave tensor from tensor of potentials. If :attr:`threshold` is :attr:`None`, all the neurons
    emit one spike (if the potential is greater than zero) in the last time step.

    Args:
        potentials (Tensor): The tensor of input potentials.
        threshold (float): Firing threshold. Default: None
        return_thresholded_potentials (boolean): If True, the tensor of thresholded potentials will be returned
        as well as the tensor of spike-wave. Default: False

    Returns:
        Tensor: Spike-wave tensor.
    &#34;&#34;&#34;
    thresholded = potentials.clone().detach()
    if threshold is None:
        thresholded[:-1]=0
    else:
        fn.threshold_(thresholded, threshold, 0)
    if return_thresholded_potentials:
        return thresholded.sign(), thresholded
    return thresholded.sign()</code></pre>
</details>
</dd>
<dt id="SpykeTorch.functional.fire_"><code class="name flex">
<span>def <span class="ident">fire_</span></span>(<span>potentials, threshold=None)</span>
</code></dt>
<dd>
<div class="desc"><p>The inplace version of :func:<code>~fire</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fire_(potentials, threshold=None):
    r&#34;&#34;&#34;The inplace version of :func:`~fire`
    &#34;&#34;&#34;
    if threshold is None:
        potentials[:-1]=0
    else:
        fn.threshold_(potentials, threshold, 0)
    potentials.sign_()</code></pre>
</details>
</dd>
<dt id="SpykeTorch.functional.threshold"><code class="name flex">
<span>def <span class="ident">threshold</span></span>(<span>potentials, threshold=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies a threshold on potentials by which all of the values lower or equal to the threshold becomes zero.
If :attr:<code><a title="SpykeTorch.functional.threshold" href="#SpykeTorch.functional.threshold">threshold()</a></code> is :attr:<code>None</code>, only the potentials corresponding to the final time step will survive.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>potentials</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>The tensor of input potentials.</dd>
<dt><strong><code>threshold</code></strong> :&ensp;<code>float</code></dt>
<dd>The threshold value. Default: None</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>Thresholded potentials.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def threshold(potentials, threshold=None):
    r&#34;&#34;&#34;Applies a threshold on potentials by which all of the values lower or equal to the threshold becomes zero.
    If :attr:`threshold` is :attr:`None`, only the potentials corresponding to the final time step will survive.

    Args:
        potentials (Tensor): The tensor of input potentials.
        threshold (float): The threshold value. Default: None

    Returns:
        Tensor: Thresholded potentials.
    &#34;&#34;&#34;
    outputs = potentials.clone().detach()
    if threshold is None:
        outputs[:-1]=0
    else:
        fn.threshold_(outputs, threshold, 0)
    return outputs</code></pre>
</details>
</dd>
<dt id="SpykeTorch.functional.threshold_"><code class="name flex">
<span>def <span class="ident">threshold_</span></span>(<span>potentials, threshold=None)</span>
</code></dt>
<dd>
<div class="desc"><p>The inplace version of :func:<code>~threshold</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def threshold_(potentials, threshold=None):
    r&#34;&#34;&#34;The inplace version of :func:`~threshold`
    &#34;&#34;&#34;
    if threshold is None:
        potentials[:-1]=0
    else:
        fn.threshold_(potentials, threshold, 0)</code></pre>
</details>
</dd>
<dt id="SpykeTorch.functional.pointwise_inhibition"><code class="name flex">
<span>def <span class="ident">pointwise_inhibition</span></span>(<span>thresholded_potentials)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs point-wise inhibition between feature maps. After inhibition, at most one neuron is allowed to fire at each
position, which is the neuron with the earliest spike time. If the spike times are the same, the neuron with the maximum
potential will be chosen. As a result, the potential of all of the inhibited neurons will be reset to zero.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>thresholded_potentials</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>The tensor of thresholded input potentials.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>Inhibited potentials.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pointwise_inhibition(thresholded_potentials):
    r&#34;&#34;&#34;Performs point-wise inhibition between feature maps. After inhibition, at most one neuron is allowed to fire at each
    position, which is the neuron with the earliest spike time. If the spike times are the same, the neuron with the maximum
    potential will be chosen. As a result, the potential of all of the inhibited neurons will be reset to zero.

    Args:
        thresholded_potentials (Tensor): The tensor of thresholded input potentials.

    Returns:
        Tensor: Inhibited potentials.
    &#34;&#34;&#34;
    # maximum of each position in each time step
    maximum = torch.max(thresholded_potentials, dim=1, keepdim=True)
    # compute signs for detection of the earliest spike
    clamp_pot = maximum[0].sign()
    # maximum of clamped values is the indices of the earliest spikes
    clamp_pot_max_1 = (clamp_pot.size(0) - clamp_pot.sum(dim = 0, keepdim=True)).long()
    clamp_pot_max_1.clamp_(0,clamp_pot.size(0)-1)
    clamp_pot_max_0 = clamp_pot[-1:,:,:,:]
    # finding winners (maximum potentials between early spikes)
    winners = maximum[1].gather(0, clamp_pot_max_1)
    # generating inhibition coefficient
    coef = torch.zeros_like(thresholded_potentials[0]).unsqueeze_(0)
    coef.scatter_(1, winners,clamp_pot_max_0)
    # applying inhibition to potentials (broadcasting multiplication)
    return torch.mul(thresholded_potentials, coef)</code></pre>
</details>
</dd>
<dt id="SpykeTorch.functional.feature_inhibition_"><code class="name flex">
<span>def <span class="ident">feature_inhibition_</span></span>(<span>potentials, inhibited_features)</span>
</code></dt>
<dd>
<div class="desc"><p>The inplace version of :func:<code>~feature_inhibition</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def feature_inhibition_(potentials, inhibited_features):
    r&#34;&#34;&#34;The inplace version of :func:`~feature_inhibition`
    &#34;&#34;&#34;
    if len(inhibited_features) != 0:
        potentials[:, inhibited_features, :, :] = 0</code></pre>
</details>
</dd>
<dt id="SpykeTorch.functional.feature_inhibition"><code class="name flex">
<span>def <span class="ident">feature_inhibition</span></span>(<span>potentials, inhibited_features)</span>
</code></dt>
<dd>
<div class="desc"><p>Inhibits specified features (reset the corresponding neurons' potentials to zero).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>potentials</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>The tensor of input potentials.</dd>
<dt><strong><code>inhibited_features</code></strong> :&ensp;<code>List</code></dt>
<dd>The list of features to be inhibited.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>Inhibited potentials.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def feature_inhibition(potentials, inhibited_features):
    r&#34;&#34;&#34;Inhibits specified features (reset the corresponding neurons&#39; potentials to zero).

    Args:
        potentials (Tensor): The tensor of input potentials.
        inhibited_features (List): The list of features to be inhibited.

    Returns:
        Tensor: Inhibited potentials.
    &#34;&#34;&#34;
    potentials_copy = potentials.clone().detach()
    if len(inhibited_features) != 0:
        feature_inhibition_(potentials_copy, inhibited_features)
    return potentials_copy</code></pre>
</details>
</dd>
<dt id="SpykeTorch.functional.get_k_winners"><code class="name flex">
<span>def <span class="ident">get_k_winners</span></span>(<span>potentials, kwta=1, inhibition_radius=0, spikes=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Finds at most :attr:<code>kwta</code> winners first based on the earliest spike time, then based on the maximum potential.
It returns a list of winners, each in a tuple of form (feature, row, column).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Winners are selected sequentially. Each winner inhibits surrounding neruons in a specific radius in all of the
other feature maps. Note that only one winner can be selected from each feature map.</p>
</div>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>potentials</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>The tensor of input potentials.</dd>
<dt><strong><code>kwta</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of winners. Default: 1</dd>
<dt><strong><code>inhibition_radius</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The radius of lateral inhibition. Default: 0</dd>
<dt><strong><code>spikes</code></strong> :&ensp;<code>Tensor</code>, optional</dt>
<dd>Spike-wave corresponding to the input potentials. Default: None</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>List</code></dt>
<dd>List of winners.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_k_winners(potentials, kwta = 1, inhibition_radius = 0, spikes = None):
    r&#34;&#34;&#34;Finds at most :attr:`kwta` winners first based on the earliest spike time, then based on the maximum potential.
    It returns a list of winners, each in a tuple of form (feature, row, column).

    .. note::

        Winners are selected sequentially. Each winner inhibits surrounding neruons in a specific radius in all of the
        other feature maps. Note that only one winner can be selected from each feature map.

    Args:
        potentials (Tensor): The tensor of input potentials.
        kwta (int, optional): The number of winners. Default: 1
        inhibition_radius (int, optional): The radius of lateral inhibition. Default: 0
        spikes (Tensor, optional): Spike-wave corresponding to the input potentials. Default: None

    Returns:
        List: List of winners.
    &#34;&#34;&#34;
    if spikes is None:
        spikes = potentials.sign()
    # finding earliest potentials for each position in each feature
    maximum = (spikes.size(0) - spikes.sum(dim = 0, keepdim=True)).long()
    maximum.clamp_(0,spikes.size(0)-1)
    values = potentials.gather(dim=0, index=maximum) # gathering values
    # propagating the earliest potential through the whole timesteps
    truncated_pot = spikes * values
    # summation with a high enough value (maximum of potential summation over timesteps) at spike positions
    v = truncated_pot.max() * potentials.size(0)
    truncated_pot.addcmul_(spikes,v)
    # summation over all timesteps
    total = truncated_pot.sum(dim=0,keepdim=True)
    
    total.squeeze_(0)
    global_pooling_size = tuple(total.size())
    winners = []
    for k in range(kwta):
        max_val,max_idx = total.view(-1).max(0)
        if max_val.item() != 0:
            # finding the 3d position of the maximum value
            max_idx_unraveled = np.unravel_index(max_idx.item(),global_pooling_size)
            # adding to the winners list
            winners.append(max_idx_unraveled)
            # preventing the same feature to be the next winner
            total[max_idx_unraveled[0],:,:] = 0
            # columnar inhibition (increasing the chance of leanring diverse features)
            if inhibition_radius != 0:
                rowMin,rowMax = max(0,max_idx_unraveled[-2]-inhibition_radius),min(total.size(-2),max_idx_unraveled[-2]+inhibition_radius+1)
                colMin,colMax = max(0,max_idx_unraveled[-1]-inhibition_radius),min(total.size(-1),max_idx_unraveled[-1]+inhibition_radius+1)
                total[:,rowMin:rowMax,colMin:colMax] = 0
        else:
            break
    return winners</code></pre>
</details>
</dd>
<dt id="SpykeTorch.functional.intensity_lateral_inhibition"><code class="name flex">
<span>def <span class="ident">intensity_lateral_inhibition</span></span>(<span>intencities, inhibition_kernel)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies lateral inhibition on intensities. For each location, this inhibition decreases the intensity of the
surrounding cells that has lower intensities by a specific factor. This factor is relative to the distance of the
neighbors and are put in the :attr:<code>inhibition_kernel</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>intencities</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>The tensor of input intensities.</dd>
<dt><strong><code>inhibition_kernel</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>The tensor of inhibition factors.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>Inhibited intensities.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intensity_lateral_inhibition(intencities, inhibition_kernel):
    r&#34;&#34;&#34;Applies lateral inhibition on intensities. For each location, this inhibition decreases the intensity of the
    surrounding cells that has lower intensities by a specific factor. This factor is relative to the distance of the
    neighbors and are put in the :attr:`inhibition_kernel`.

    Args:
        intencities (Tensor): The tensor of input intensities.
        inhibition_kernel (Tensor): The tensor of inhibition factors.

    Returns:
        Tensor: Inhibited intensities.
    &#34;&#34;&#34;
    intencities.squeeze_(0)
    intencities.unsqueeze_(1)

    inh_win_size = inhibition_kernel.size(-1)
    rad = inh_win_size//2
    # repeat each value
    values = intencities.reshape(intencities.size(0),intencities.size(1),-1,1)
    values = values.repeat(1,1,1,inh_win_size)
    values = values.reshape(intencities.size(0),intencities.size(1),-1,intencities.size(-1)*inh_win_size)
    values = values.repeat(1,1,1,inh_win_size)
    values = values.reshape(intencities.size(0),intencities.size(1),-1,intencities.size(-1)*inh_win_size)
    # extend patches
    padded = fn.pad(intencities,(rad,rad,rad,rad))
    # column-wise
    patches = padded.unfold(-1,inh_win_size,1)
    patches = patches.reshape(patches.size(0),patches.size(1),patches.size(2),-1,patches.size(3)*patches.size(4))
    patches.squeeze_(-2)
    # row-wise
    patches = patches.unfold(-2,inh_win_size,1).transpose(-1,-2)
    patches = patches.reshape(patches.size(0),patches.size(1),1,-1,patches.size(-1))
    patches.squeeze_(-3)
    # compare each element by its neighbors
    coef = values - patches
    coef.clamp_(min=0).sign_() # &#34;ones&#34; are neighbors greater than center
    # convolution with full stride to get accumulative inhibiiton factor
    factors = fn.conv2d(coef, inhibition_kernel, stride=inh_win_size)
    result = intencities + intencities * factors

    intencities.squeeze_(1)
    intencities.unsqueeze_(0)
    result.squeeze_(1)
    result.unsqueeze_(0)
    return result</code></pre>
</details>
</dd>
<dt id="SpykeTorch.functional.local_normalization"><code class="name flex">
<span>def <span class="ident">local_normalization</span></span>(<span>input, normalization_radius, eps=1e-12)</span>
</code></dt>
<dd>
<div class="desc"><p>Applies local normalization. on each region (of size radius*2 + 1) the mean value is computed and the
intensities will be divided by the mean value. The input is a 4D tensor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>The input tensor of shape (timesteps, features, height, width).</dd>
<dt><strong><code>normalization_radius</code></strong> :&ensp;<code>int</code></dt>
<dd>The radius of normalization window.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tensor</code></dt>
<dd>Locally normalized tensor.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def local_normalization(input, normalization_radius, eps=1e-12):
    r&#34;&#34;&#34;Applies local normalization. on each region (of size radius*2 + 1) the mean value is computed and the
    intensities will be divided by the mean value. The input is a 4D tensor.

    Args:
        input (Tensor): The input tensor of shape (timesteps, features, height, width).
        normalization_radius (int): The radius of normalization window.

    Returns:
        Tensor: Locally normalized tensor.
    &#34;&#34;&#34;
    # computing local mean by 2d convolution
    kernel = torch.ones(1,1,normalization_radius*2+1,normalization_radius*2+1,device=input.device).float()/((normalization_radius*2+1)**2)
    # rearrange 4D tensor so input channels will be considered as minibatches
    y = input.squeeze(0) # removes minibatch dim which was 1
    y.unsqueeze_(1)  # adds a dimension after channels so previous channels are now minibatches
    means = fn.conv2d(y,kernel,padding=normalization_radius) + eps # computes means
    y = y/means # normalization
    # swap minibatch with channels
    y.squeeze_(1)
    y.unsqueeze_(0)
    return y</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="SpykeTorch" href="index.html">SpykeTorch</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="SpykeTorch.functional.pad" href="#SpykeTorch.functional.pad">pad</a></code></li>
<li><code><a title="SpykeTorch.functional.pooling" href="#SpykeTorch.functional.pooling">pooling</a></code></li>
<li><code><a title="SpykeTorch.functional.fire" href="#SpykeTorch.functional.fire">fire</a></code></li>
<li><code><a title="SpykeTorch.functional.fire_" href="#SpykeTorch.functional.fire_">fire_</a></code></li>
<li><code><a title="SpykeTorch.functional.threshold" href="#SpykeTorch.functional.threshold">threshold</a></code></li>
<li><code><a title="SpykeTorch.functional.threshold_" href="#SpykeTorch.functional.threshold_">threshold_</a></code></li>
<li><code><a title="SpykeTorch.functional.pointwise_inhibition" href="#SpykeTorch.functional.pointwise_inhibition">pointwise_inhibition</a></code></li>
<li><code><a title="SpykeTorch.functional.feature_inhibition_" href="#SpykeTorch.functional.feature_inhibition_">feature_inhibition_</a></code></li>
<li><code><a title="SpykeTorch.functional.feature_inhibition" href="#SpykeTorch.functional.feature_inhibition">feature_inhibition</a></code></li>
<li><code><a title="SpykeTorch.functional.get_k_winners" href="#SpykeTorch.functional.get_k_winners">get_k_winners</a></code></li>
<li><code><a title="SpykeTorch.functional.intensity_lateral_inhibition" href="#SpykeTorch.functional.intensity_lateral_inhibition">intensity_lateral_inhibition</a></code></li>
<li><code><a title="SpykeTorch.functional.local_normalization" href="#SpykeTorch.functional.local_normalization">local_normalization</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>